{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@important note: \\n    the script is altered by ChenglongChen 3rd place solution for HomeDepot product search \\n    results relevance competition on Kaggle.\\n@author: Eric Tsai <eric492718@gmail.com>\\n@brief: process data\\n        - a bunck of processing\\n        - automated spelling correction\\n        - query expansion\\n        - extract product name for search_term and product_title\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@important note: \n",
    "    the script is altered by ChenglongChen 3rd place solution for HomeDepot product search \n",
    "    results relevance competition on Kaggle.\n",
    "@author: Eric Tsai <eric492718@gmail.com>\n",
    "@brief: process data\n",
    "        - a bunck of processing\n",
    "        - automated spelling correction\n",
    "        - query expansion\n",
    "        - extract product name for search_term and product_title\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# utils\n",
    "import csv  \n",
    "from importlib.machinery import SourceFileLoader\n",
    "import multiprocessing\n",
    "from bs4 import BeautifulSoup  # 處理 html tag\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "import nltk  # 一套基於 Python 的自然語言處理工具箱\n",
    "import regex  # it is necessary because using the re module will get an error\n",
    "\n",
    "# libraries we write\n",
    "import config\n",
    "from utils import ngram_utils, pkl_utils, logging_utils, time_utils\n",
    "from spelling_checker import GoogleQuerySpellingChecker, AutoSpellingChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use the multiprocessing package, I must put all related function in the script file, and import it\n",
    "import parallel_processing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "  <strong>Note!</strong> <br>在Python的string前面加上‘r’， 是為了告訴編譯器這個string是個raw string，不要轉意backslash '\\' 。 例如，\\n 在raw string中，是兩個字元，\\和n， 而不會轉意為換行符。由於正則表示式和 \\ 會有衝突，因此，當一個字串使用了正則表示式後，最好在前面加上'r'。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "                   BaseReplacer()  ==>   Attribute: pattern_replace_pair_list=[], Method: transform(text)\n",
    "                         /                   \\\n",
    "                        /                     \\\n",
    "              LowerCaseConverter      LowerUpperCaseSplitter\n",
    "             (method overriding)      (attribute overriding)\n",
    "                        \\                     /\n",
    "                         \\                   /\n",
    "                                   C        \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "script"
    ]
   },
   "outputs": [],
   "source": [
    "#--------------------------- Processor ---------------------------\n",
    "## base class\n",
    "## Most of the processings can be casted into the \"pattern-replace\" framework\n",
    "class BaseReplacer:\n",
    "    '''\n",
    "    re.sub(pattern,repl,string,count)\n",
    "    pattern: pattern which we want to replace. It write by regular expression.\n",
    "    repl:replacer\n",
    "    string: the string we need to deal with\n",
    "    count:number of match pattern that we want to replace. If I choose 0, it means replacing all the targets.\n",
    "    '''\n",
    "    def __init__(self, pattern_replace_pair_list=[]):\n",
    "        self.pattern_replace_pair_list = pattern_replace_pair_list\n",
    "    def transform(self, text):\n",
    "        for pattern, replace in self.pattern_replace_pair_list:\n",
    "            try:\n",
    "                text = regex.sub(pattern, replace, text)\n",
    "            except ValueError:\n",
    "                print(ValueError)\n",
    "        return regex.sub(r'\\s+', ' ', text).strip() # \\s+ : means \"one or more spaces\" replace to one space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'Eric  Tsai Tsai Tsai Tsai is good'\n",
    "# pattern_replace_pair_list = [('Tsai', 'Eric'), ('good', 'bad')]\n",
    "# print(text)\n",
    "# BaseReplacer(pattern_replace_pair_list).transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_BaseReplacer():\n",
    "    text = 'Eric  Tsai Tsai Tsai Tsai is good'\n",
    "    pattern_replace_pair_list = [('Tsai', 'Eric'), ('good', 'bad')]\n",
    "    assert BaseReplacer(pattern_replace_pair_list).transform(text)=='Eric Eric Eric Eric Eric is bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "script"
    ]
   },
   "outputs": [],
   "source": [
    "## deal with case\n",
    "# Inheritance BaseReplacer Attribute and Method\n",
    "class LowerCaseConverter(BaseReplacer):\n",
    "    \"\"\"\n",
    "    Traditional -> traditional\n",
    "    \"\"\"\n",
    "    # Method Overriding (方法覆寫)，覆寫從 BaseReplacer 繼承的方法\n",
    "    def transform(self, text):\n",
    "        return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'EricTsai Tsai Tsai Tsai is good'\n",
    "# print(text)\n",
    "# LowerCaseConverter().transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LowerCaseConverter():\n",
    "    text = 'EricTsai Tsai Tsai Tsai is good'\n",
    "    assert LowerCaseConverter().transform(text) == 'erictsai tsai tsai tsai is good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "script"
    ]
   },
   "outputs": [],
   "source": [
    "class LowerUpperCaseSplitter(BaseReplacer):\n",
    "    \"\"\"\n",
    "    homeBASICS Traditional Real Wood -> homeBASICS Traditional Real Wood\n",
    "\n",
    "    hidden from viewDurable rich finishLimited lifetime warrantyEncapsulated panels ->\n",
    "    hidden from view Durable rich finish limited lifetime warranty Encapsulated panels\n",
    "\n",
    "    Dickies quality has been built into every product.Excellent visibilityDurable ->\n",
    "    Dickies quality has been built into every product Excellent visibility Durable\n",
    "\n",
    "    BAD CASE:\n",
    "    shadeMature height: 36 in. - 48 in.Mature width\n",
    "    minutesCovers up to 120 sq. ft.Cleans up\n",
    "    PUT one UnitConverter before LowerUpperCaseSplitter\n",
    "\n",
    "    Reference:\n",
    "    https://www.kaggle.com/c/home-depot-product-search-relevance/forums/t/18472/typos-in-the-product-descriptions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        ########################################################################\n",
    "        # The first regular expression can solve the problem which is a \n",
    "        # sentence connect with the other sentence but without any character.\n",
    "        ########################################################################\n",
    "        # The second regular expression means: a Blank characters, followed \n",
    "        # by the low case letter, followed by a upper case letter character.\n",
    "        self.pattern_replace_pair_list = [(r'(\\w)[\\.?!]([A-Z])', r'\\1 \\2'), # \\1: means the group 1 element. In this case, it is items which match with pattern (\\w)  \n",
    "                                          (r'(?<=( ))([a-z]+)([A-Z]+)', r'\\2 \\3'),]  # \\2: means the group 2 element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show some example\n",
    "# display( LowerUpperCaseSplitter().transform('homeBASICS Traditional Real Wood') )\n",
    "# display( LowerUpperCaseSplitter().transform('hidden from viewDurable rich finishLimited lifetime warrantyEncapsulated panels') )\n",
    "# ##############################################################################################################################\n",
    "# display( LowerUpperCaseSplitter().transform('shadeMature height: 36 in. - 48 in.Mature width') )\n",
    "# # if add a blank character in the beginning, the function will work \n",
    "# display( LowerUpperCaseSplitter().transform(' shadeMature height: 36 in. - 48 in.Mature width') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LowerUpperCaseSplitter():\n",
    "    assert (LowerUpperCaseSplitter().transform('homeBASICS Traditional Real Wood')\n",
    "            == 'homeBASICS Traditional Real Wood')\n",
    "    assert (LowerUpperCaseSplitter().transform('hidden from viewDurable rich finishLimited lifetime warrantyEncapsulated panels') \n",
    "            == 'hidden from view Durable rich finish Limited lifetime warranty Encapsulated panels')\n",
    "    assert (LowerUpperCaseSplitter().transform('shadeMature height: 36 in. - 48 in.Mature width')\n",
    "            == 'shadeMature height: 36 in. - 48 in Mature width')\n",
    "    assert (LowerUpperCaseSplitter().transform(' shadeMature height: 36 in. - 48 in.Mature width')\n",
    "           =='shade Mature height: 36 in. - 48 in Mature width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create word replacement patterns, using homemade replacement words.\n",
    "Input will be a CSV file that contains one column and each value is\n",
    "a text which shows the words, followed by \",\", and followed by a word we\n",
    "want to replace. Note, texts is annotated if it begins with '#' character.\n",
    "'''\n",
    "class WordReplacer(BaseReplacer):\n",
    "    '''\n",
    "    if words are not near the [a-z0-9_](\\W or ^\\w), \n",
    "    replce it by replacement dictionary\n",
    "    '''\n",
    "    def __init__(self, replace_fname):\n",
    "        self.replace_fname = replace_fname # file name which is alread create in config\n",
    "        self.pattern_replace_pair_list = []\n",
    "        for line in csv.reader(open(self.replace_fname)): # use csv.reader will return a list which seperate by \",\"\n",
    "            if len(line) == 1 or line[0].startswith('#'):\n",
    "                continue # The continue statement is used to skip the rest of the code inside a loop for the current iteration only. Loop does not terminate but continues on with the next iteration. \n",
    "            try: # Regular Expression means: a text is between two characters which are non-alphanumeric characters \n",
    "                pattern = r'(?<=\\W|^)%s(?=\\W|$)'%line[0] # or a text is first character in the string (^)\n",
    "                replace = line[1]                        # or a text is the end of the string ($)\n",
    "                self.pattern_replace_pair_list.append( (pattern, replace) )\n",
    "            except:\n",
    "                print(line)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display how to use csv.reader\n",
    "# i = -1\n",
    "# for line in csv.reader(open('../Data/dict/word_replacer.csv')):\n",
    "#     i=i+1\n",
    "#     if i == 1:\n",
    "#         print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_list = WordReplacer(replace_fname=config.WORD_REPLACER_DATA).pattern_replace_pair_list\n",
    "# replace_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show some example\n",
    "# text1 = regex.sub(replace_list[0][0], replace_list[0][1],'Eric Tsai want a undercabinet')\n",
    "# text2 = regex.sub(replace_list[0][0], replace_list[0][1],'Eric Tsai want a 2undercabinet')\n",
    "# text3 = regex.sub(replace_list[0][0], replace_list[0][1],'Eric Tsai want a $undercabinet')\n",
    "# text4 = regex.sub(replace_list[0][0], replace_list[0][1],'undercabinet is what you need')\n",
    "# text5 = regex.sub(replace_list[0][0], replace_list[0][1],'The undercabinet is what you need')\n",
    "# text6 = regex.sub(replace_list[0][0], replace_list[0][1],'*undercabinet is what you need')\n",
    "# text7 = regex.sub(replace_list[0][0], replace_list[0][1],'undercabinet% is what you need')\n",
    "# text8 = regex.sub(replace_list[0][0], replace_list[0][1],'undercabinet_ is what you need')\n",
    "# display(text1, text2, text3, text4, text5, text6, text7, text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_WordReplacer():\n",
    "    replace_list = WordReplacer(replace_fname=config.WORD_REPLACER_DATA).pattern_replace_pair_list\n",
    "    assert regex.sub(replace_list[0][0], replace_list[0][1],'Eric Tsai want a undercabinet') == 'Eric Tsai want a under cabinet'\n",
    "    assert regex.sub(replace_list[0][0], replace_list[0][1],'Eric Tsai want a 2undercabinet') == 'Eric Tsai want a 2undercabinet'\n",
    "    assert regex.sub(replace_list[0][0], replace_list[0][1],'Eric Tsai want a $undercabinet') == 'Eric Tsai want a $under cabinet'\n",
    "    assert regex.sub(replace_list[0][0], replace_list[0][1],'undercabinet is what you need') == 'under cabinet is what you need'\n",
    "    assert regex.sub(replace_list[0][0], replace_list[0][1],'The undercabinet is what you need') == 'The under cabinet is what you need'\n",
    "    assert regex.sub(replace_list[0][0], replace_list[0][1],'*undercabinet is what you need') == '*under cabinet is what you need'\n",
    "    assert regex.sub(replace_list[0][0], replace_list[0][1],'undercabinet% is what you need') == 'under cabinet% is what you need'\n",
    "    assert regex.sub(replace_list[0][0], replace_list[0][1],'undercabinet_ is what you need') == 'undercabinet_ is what you need'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">***The class below has a bug in Chenglong version. The pattern was wrong. But I already took care of it. The original version can't deal with many '-' characters in the text like 'Vinyl-Leather-Rubber'.***</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with letters\n",
    "class LetterLetterSplitter(BaseReplacer):\n",
    "    \"\"\"\n",
    "    For letter and letter\n",
    "    /:\n",
    "    Cleaner/Conditioner -> Cleaner Conditioner\n",
    "\n",
    "    -:\n",
    "    Vinyl-Leather-Rubber -> Vinyl Leather Rubber\n",
    "\n",
    "    For digit and digit, we keep it as we will generate some features via math operations,\n",
    "    such as approximate height/width/area etc.\n",
    "    /:\n",
    "    3/4 -> 3/4\n",
    "\n",
    "    -:\n",
    "    1-1/4 -> 1-1/4\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r'(?<=[a-zA-Z])[/\\-](?=[a-zA-Z])', r' ')\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show some example\n",
    "# display(\n",
    "#     LetterLetterSplitter().transform('Cleaner/Conditioner'),\n",
    "#     LetterLetterSplitter().transform('Vinyl-Leather-Rubber'),\n",
    "#     LetterLetterSplitter().transform('Vinyl-Leather/Rubber'),\n",
    "#     LetterLetterSplitter().transform('COVID-19 is crazy'),\n",
    "#     LetterLetterSplitter().transform('3/4'),\n",
    "#     LetterLetterSplitter().transform('1-1/4'),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LetterLetterSplitter():\n",
    "    assert LetterLetterSplitter().transform('Cleaner/Conditioner') == 'Cleaner Conditioner'\n",
    "    assert LetterLetterSplitter().transform('Vinyl-Leather-Rubber') == 'Vinyl Leather Rubber'\n",
    "    assert LetterLetterSplitter().transform('Vinyl-Leather/Rubber') == 'Vinyl Leather Rubber'\n",
    "    assert LetterLetterSplitter().transform('COVID-19 is crazy') == 'COVID-19 is crazy'\n",
    "    assert LetterLetterSplitter().transform('3/4') == '3/4'\n",
    "    assert LetterLetterSplitter().transform('1-1/4') == '1-1/4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with digits and numbers\n",
    "class DigitLetterSplitter(BaseReplacer):\n",
    "    \"\"\"\n",
    "    x:\n",
    "    1x1x1x1x1 -> 1 x 1 x 1 x 1 x 1\n",
    "    19.875x31.5x1 -> 19.875 x 31.5 x 1\n",
    "\n",
    "    -:\n",
    "    1-Gang -> 1 Gang\n",
    "    48-Light -> 48 Light\n",
    "\n",
    "    .:\n",
    "    includes a tile flange to further simplify installation.60 in. L x 36 in. W x 20 in. ->\n",
    "    includes a tile flange to further simplify installation. 60 in. L x 36 in. W x 20 in.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r'(\\d+)[\\.\\-]*([a-zA-Z]+)', r'\\1 \\2'),\n",
    "            (r'([a-zA-Z]+)[\\.\\-]*(\\d+)', r'\\1 \\2'),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show some example\n",
    "# display(\n",
    "#     DigitLetterSplitter().transform('1.a'),\n",
    "#     DigitLetterSplitter().transform('1x'),\n",
    "#     DigitLetterSplitter().transform('1x1x1x1x1'),\n",
    "#     DigitLetterSplitter().transform('1-Gang'),\n",
    "#     DigitLetterSplitter().transform('COVID-19 is crazy'),\n",
    "#     DigitLetterSplitter().transform('3/4'),\n",
    "#     DigitLetterSplitter().transform('1-1/4'),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DigitLetterSplitter():\n",
    "    assert DigitLetterSplitter().transform('1.a') == '1 a'\n",
    "    assert DigitLetterSplitter().transform('1x') == '1 x'\n",
    "    assert DigitLetterSplitter().transform('1x1x1x1x1') == '1 x 1 x 1 x 1 x 1'\n",
    "    assert DigitLetterSplitter().transform('1-Gang') == '1 Gang'\n",
    "    assert DigitLetterSplitter().transform('COVID-19 is crazy') == 'COVID 19 is crazy'\n",
    "    assert DigitLetterSplitter().transform('3/4') == '3/4'\n",
    "    assert DigitLetterSplitter().transform('1-1/4') == '1-1/4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCommaDigitMerger(BaseReplacer):\n",
    "    \"\"\"\n",
    "    1,000,000 -> 1000000\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r\"(?<=\\d+),(?=000)\", r\"\"),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show some example\n",
    "# display(\n",
    "#     DigitCommaDigitMerger().transform('1,000,000'),\n",
    "#     DigitCommaDigitMerger().transform('900,000'),\n",
    "#     DigitCommaDigitMerger().transform('80,000'),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DigitCommaDigitMerger():\n",
    "    assert DigitCommaDigitMerger().transform('1,000,000') == '1000000'\n",
    "    assert DigitCommaDigitMerger().transform('900,000') == '900000'\n",
    "    assert DigitCommaDigitMerger().transform('80,000') == '80000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberDigitMapper(BaseReplacer):\n",
    "    \"\"\"\n",
    "    one -> 1\n",
    "    two -> 2\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        numbers = [\n",
    "            'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "            'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen',\n",
    "            'nineteen', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety'\n",
    "        ]\n",
    "        digits = [\n",
    "            0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n",
    "            16, 17, 18, 19, 20, 30, 40, 50, 60, 70, 80, 90\n",
    "        ]\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r'(?<=\\W|^)%s(?=\\W|$)'%n, str(d)) for n,d in zip(numbers, digits)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show some example\n",
    "# display(\n",
    "#     NumberDigitMapper().transform('one'),\n",
    "#     NumberDigitMapper().transform('ten'),\n",
    "#     NumberDigitMapper().transform('fifty'),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_NumberDigitMapper():\n",
    "    assert NumberDigitMapper().transform('one') == '1'\n",
    "    assert NumberDigitMapper().transform('ten') == '10'\n",
    "    assert NumberDigitMapper().transform('fifty') == '50'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">***The class below has a bug in Chenglong version. The pattern of regular expression was wrong. But I already took care of it. The original version can't deal with words that contain 'in' like 100 instergram accounts. I already fixed it. But still can't deal with digital followed by an adposition 'in'. Like 'What is 100 in Chinese?'***</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with unit\n",
    "class UnitConverter(BaseReplacer):\n",
    "    \"\"\"\n",
    "    shadeMature height: 36 in. - 48 in.Mature width\n",
    "    PUT one UnitConverter before LowerUpperCaseSplitter\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r\"([0-9]+)( *)(inches|inch|in|in\\.|')(?=[^\\w]+)\\.?\", r\"\\1 in. \"),\n",
    "            (r\"([0-9]+)( *)(pounds|pound|lbs|lb|lb\\.)(?=[^\\w]+)\\.?\", r\"\\1 lb. \"),\n",
    "            (r\"([0-9]+)( *)(foot|feet|ft|ft\\.|'')(?=[^\\w]+)\\.?\", r\"\\1 ft. \"),\n",
    "            (r\"([0-9]+)( *)(square|sq|sq\\.) ?\\.?(inches|inch|in|in.|')(?=[^\\w]+)\\.?\", r\"\\1 sq.in. \"),\n",
    "            (r\"([0-9]+)( *)(square|sq|sq\\.) ?\\.?(feet|foot|ft|ft.|'')(?=[^\\w]+)\\.?\", r\"\\1 sq.ft. \"),\n",
    "            (r\"([0-9]+)( *)(cubic|cu|cu\\.) ?\\.?(inches|inch|in|in.|')(?=[^\\w]+)\\.?\", r\"\\1 cu.in. \"),\n",
    "            (r\"([0-9]+)( *)(cubic|cu|cu\\.) ?\\.?(feet|foot|ft|ft.|'')(?=[^\\w]+)\\.?\", r\"\\1 cu.ft. \"),\n",
    "            (r\"([0-9]+)( *)(gallons|gallon|gal)(?=[^\\w]+)\\.?\", r\"\\1 gal. \"),\n",
    "            (r\"([0-9]+)( *)(ounces|ounce|oz)(?=[^\\w]+)\\.?\", r\"\\1 oz. \"),\n",
    "            (r\"([0-9]+)( *)(centimeters|cm)(?=[^\\w]+)\\.?\", r\"\\1 cm. \"),\n",
    "            (r\"([0-9]+)( *)(milimeters|mm)(?=[^\\w]+)\\.?\", r\"\\1 mm. \"),\n",
    "            (r\"([0-9]+)( *)(minutes|minute)(?=[^\\w]+)\\.??\", r\"\\1 min. \"),\n",
    "            (r\"([0-9]+)( *)(°|degrees|degree)(?=[^\\w]+)\\.?\", r\"\\1 deg. \"),\n",
    "            (r\"([0-9]+)( *)(v|volts|volt)(?=[^\\w]+)(?=[^\\w]+)\\.?\", r\"\\1 volt. \"),\n",
    "            (r\"([0-9]+)( *)(wattage|watts|watt)(?=[^\\w]+)\\.?\", r\"\\1 watt. \"),\n",
    "            (r\"([0-9]+)( *)(amperes|ampere|amps|amp)(?=[^\\w]+)\\.?\", r\"\\1 amp. \"),\n",
    "            (r\"([0-9]+)( *)(qquart|quart)(?=[^\\w]+)\\.?\", r\"\\1 qt. \"),\n",
    "            (r\"([0-9]+)( *)(hours|hour|hrs\\.)(?=[^\\w]+)\\.?\", r\"\\1 hr \"),\n",
    "            (r\"([0-9]+)( *)(gallons per minute|gallon per minute|gal per minute|gallons/min.|gallons/min)(?=[^\\w]+)\\.?\", r\"\\1 gal. per min. \"),\n",
    "            (r\"([0-9]+)( *)(gallons per hour|gallon per hour|gal per hour|gallons/hour|gallons/hr)(?=[^\\w]+)\\.?\", r\"\\1 gal. per hr \"),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show some example\n",
    "# display(\n",
    "#     UnitConverter().transform('shadeMature height: 36 in. - 48 in.Mature width'),\n",
    "#     UnitConverter().transform('shadeMature height: 36 in - 48 in.Mature width'),\n",
    "#     UnitConverter().transform('shadeMature height: 36 inch - 48 in.Mature width'),\n",
    "#     UnitConverter().transform('shadeMature height: 36in. - 48in.Mature width'),\n",
    "#     UnitConverter().transform('shadeMature height: 36in - 48in Mature width'),\n",
    "#     UnitConverter().transform('shadeMature height: 36inyy - 48in Mature width'),\n",
    "#     UnitConverter().transform(\"shadeMature height: 36' - 48in Mature width\"),  # \"inch\" abbreviation:  \"'\" or in\n",
    "#     UnitConverter().transform('Top 100 instagram Business Accounts sorted by Followers'),  # this case is not well\n",
    "#     UnitConverter().transform('100 inches is long'),\n",
    "#     UnitConverter().transform('100 inches is 50 cmiii uu'),\n",
    "#     UnitConverter().transform('100 vampire is cool'),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_UnitConverter():\n",
    "    assert UnitConverter().transform('shadeMature height: 36 in. - 48 in.Mature width') == 'shadeMature height: 36 in. - 48 in. Mature width'\n",
    "    assert UnitConverter().transform('shadeMature height: 36 in - 48 in.Mature width') == 'shadeMature height: 36 in. - 48 in. Mature width'\n",
    "    assert UnitConverter().transform('shadeMature height: 36 inch - 48 in.Mature width') == 'shadeMature height: 36 in. - 48 in. Mature width'\n",
    "    assert UnitConverter().transform('shadeMature height: 36in. - 48in.Mature width') == 'shadeMature height: 36 in. - 48 in. Mature width'\n",
    "    assert UnitConverter().transform('shadeMature height: 36in - 48in Mature width') == 'shadeMature height: 36 in. - 48 in. Mature width'\n",
    "    assert UnitConverter().transform('shadeMature height: 36inyy - 48in Mature width') == 'shadeMature height: 36inyy - 48 in. Mature width'\n",
    "    assert UnitConverter().transform(\"shadeMature height: 36' - 48in Mature width\") == 'shadeMature height: 36 in. - 48 in. Mature width'\n",
    "    assert UnitConverter().transform('Top 100 instagram Business Accounts sorted by Followers') == 'Top 100 instagram Business Accounts sorted by Followers'\n",
    "    assert UnitConverter().transform('100 inches is long') == '100 in. is long'\n",
    "    assert UnitConverter().transform('100 inches is 50 cmiii uu') == '100 in. is 50 cmiii uu'\n",
    "    assert UnitConverter().transform('100 vampire is cool') == '100 vampire is cool'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test why `UnitConverter()` must be convert text before using `LowerUpperCaseSplitter()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">***You can find out if poundFit(assume is a brand name) is a word, using `UnitConverter()` won't convert 'pound' to unit.***</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case1. `Using UnitConverter()` first\n",
    "```python\n",
    "text = 'shadeMature height: 36 in. - 48 poundFit width'\n",
    "text2 = UnitConverter().transform(text)\n",
    "text3 = LowerUpperCaseSplitter().transform(text2)\n",
    "display(text2, text3)\n",
    "\n",
    "output:\n",
    "    'shadeMature height: 36 in. - 48 poundFit width'\n",
    "    'shadeMature height: 36 in. - 48 pound Fit width\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case2. `LowerUpperCaseSplitter()` first\n",
    "```python\n",
    "text = 'shadeMature height: 36 in. - 48 poundFit width'\n",
    "text2 = LowerUpperCaseSplitter().transform(text)\n",
    "text3 = UnitConverter().transform(text2)\n",
    "\n",
    "output:\n",
    "    'shadeMature height: 36 in. - 48 pound Fit width'\n",
    "    'shadeMature height: 36 in. - 48 lb. Fit width'   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with html tags\n",
    "class HtmlCleaner:\n",
    "    def __init__(self, parser):\n",
    "        self.parser = parser  # 'html.parser'\n",
    "\n",
    "    def transform(self, text):\n",
    "        bs = BeautifulSoup(text, self.parser)\n",
    "        text = bs.get_text(separator=' ')\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show some example\n",
    "# text = '''<p>Hi. This is a simple example.<br>Yet poweful one.<p><a href=\"http://example.com/\">I linked to <i>example.com</i></a>'''\n",
    "# HtmlCleaner(parser='html.parser').transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_HtmlCleaner():\n",
    "    text = '''<p>Hi. This is a simple example.<br>Yet poweful one.<p><a href=\"http://example.com/\">I linked to <i>example.com</i></a>'''\n",
    "    assert HtmlCleaner(parser='html.parser').transform(text) == 'Hi. This is a simple example. Yet poweful one. I linked to  example.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with some special characters\n",
    "# 3rd solution in CrowdFlower (Create by Chenglong)\n",
    "class QuartetCleaner(BaseReplacer):\n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "            (r'<.+?>', r''),  # <at least one characters)\n",
    "            # html codes（character entities）\n",
    "            (r'&nbsp;', r' '),  # Non-Breakable Space\n",
    "            (r'&amp;', r'&'),  # &amp; is the character reference for \"An ampersand\".\n",
    "            (r'&#39;', r\"'\"),\n",
    "            (r'/>/Agt/>', r''),\n",
    "            (r'</a<gt/', r''),\n",
    "            (r'gt/>', r''),\n",
    "            (r'/>', r''),\n",
    "            (r'<br', r''),\n",
    "            # do not remove ['.', '/', '-', '%'] as they are useful in numbers, e.g., 1.97, 1-1/2, 10%, etc.\n",
    "            (r'[&<>)(_,;:!?\\+^~@#\\$]+', r' '),  # in the Chenglong, will remove space, but in this case no need\n",
    "            (\"'s\\\\b\", r''),\n",
    "            (r\"[']+\", r''),\n",
    "            (r'[\\\"]+', r''),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_QuartetCleaner():\n",
    "    '''\n",
    "    unit test for specific tasks and whole function\n",
    "    '''\n",
    "    # specific task\n",
    "    text = '<remove me> Hello my friends!<a>'\n",
    "    assert regex.sub(r'<.+?>', r'', text) == ' Hello my friends!'\n",
    "    text = 'Hello&nbsp;my friends!<a>'\n",
    "    assert regex.sub(r'&nbsp;', r' ', text) == 'Hello my friends!<a>'\n",
    "    text = 'Hello my friends &amp; mentor'\n",
    "    assert regex.sub(r'&amp;', r'&', text) == 'Hello my friends & mentor'\n",
    "    text = 'I&#39;m a man'\n",
    "    assert regex.sub(r'&#39;', r\"'\", text) == \"I'm a man\"\n",
    "    text = '/>/Agt/> must be remove'\n",
    "    assert regex.sub(r'/>/Agt/>', r'', text) == ' must be remove'\n",
    "    text = '</a<gt/ must be remove'\n",
    "    assert regex.sub(r'</a<gt/', r'', text) == ' must be remove'\n",
    "    text = 'gt/> must be remove'\n",
    "    assert regex.sub(r'gt/>', r'', text) == ' must be remove'\n",
    "    text = '/> must be remove'\n",
    "    assert regex.sub(r'/>', r'', text) == ' must be remove'\n",
    "    text = '<br must be remove'\n",
    "    assert regex.sub(r'<br', r'', text) == ' must be remove'\n",
    "    text = '&<>)(_,;:!?+^~@#$ must be remove'\n",
    "    assert regex.sub(r'[&<>)(_,;:!?\\+^~@#\\$]+', r'', text) == ' must be remove'\n",
    "    text = \"'s\\\\b must be remove\"\n",
    "    assert regex.sub(r\"'s\\\\b\", r'', text) == ' must be remove'\n",
    "    text = \"I'm Eric\"\n",
    "    assert regex.sub(r\"[']+\", r'', text) == 'Im Eric'\n",
    "    text = '\"Eric Tsai\" is my name.'\n",
    "    assert regex.sub(r'[\\\"]+', r'', text) == 'Eric Tsai is my name.'\n",
    "    # whole function\n",
    "    text = '<remove me> Hello my friends!<a>'\n",
    "    assert QuartetCleaner().transform(text) == 'Hello my friends'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatisation<br>\n",
    "詞性還原"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">***You can find out if poundFit(assume is a brand name) is a word, using `UnitConverter()` won't convert 'pound' to unit.***</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">***Future Improvements:<br>Add the part of speech(pos) parameter in `nltk.stem.wordnet.WordNetLemmatizer().lemmatize(token,pos)`. Otherwise, it only deal with word which pos is noun. EX, convert plural nouns to noun.***</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatizing for using pretrained word2vec model\n",
    "# 2nd solution in CrowdFlower\n",
    "class Lemmatizer:\n",
    "    '''\n",
    "    can delete white space and \\n(new line character)\n",
    "    nltk.stem.wordnet.WordNetLemmatizer().lemmatize(token):\n",
    "        You need to manually specify the part of speech(pos). \n",
    "        If you don't set the pos parameter that the default is noun, so only plural nouns can be converged here.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.Tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "        self.Lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "    def transform(self, text):\n",
    "        tokens = [self.Lemmatizer.lemmatize(token) for token in self.Tokenizer.tokenize(text)]  #cut word and do Lemmatisation\n",
    "        return ' '.join(tokens) # 'a b c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = '''\n",
    "#         I found my computers yesterday.\n",
    "#         It took me along\\n time. \n",
    "#         You guys know, a data scientist\\n has many computers which is a normal thing.\n",
    "#         By the way, I worked out and running yesterday.\n",
    "#         I hope is, are, and been can transform to be.\n",
    "#         Plural nouns: dishes, cities, knives, beliefs, heroes, volcanoes, children, crises\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer().transform(text = s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Lemmatizer():\n",
    "    text = '''I found my computers yesterday.\n",
    "              It took me along\\n time. \n",
    "              You guys know, a data scientist\\n has many computers which is a normal thing.\n",
    "              By the way, I worked out to fit my body yesterday.\n",
    "              I hope is, are, and been can transform to be.\n",
    "              dishes cities  knives beliefs heroes volcanoes'''\n",
    "    assert Lemmatizer().transform(text) == 'I found my computer yesterday. It took me along time. You guy know , a data scientist ha many computer which is a normal thing. By the way , I worked out to fit my body yesterday. I hope is , are , and been can transform to be. dish city knife belief hero volcano'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Porter Stemmer<br>\n",
    "這種詞幹算法比較舊。它是從 20 世紀 80 年代開始的，其主要關注點是刪除單詞的共同結尾，以便將它們解析爲通用形式。它不是太複雜，它的開發停止了。\n",
    "通常情況下，它是一個很好的起始基本詞幹分析器，但並不建議將它用於複雜的應用。相反，它在研究中作爲一種很好的基本詞幹算法，可以保證重複性。與其他算法相比，它也是一種非常溫和的詞幹算法。\n",
    "* Snowball Stemmer<br>\n",
    "種算法也稱爲 Porter2 詞幹算法。它幾乎被普遍認爲比 Porter 更好，甚至發明 Porter 的開發者也這麼認爲。Snowball 在 Porter 的基礎上加了很多優化。Snowball 與 Porter 相比差異約爲 5％。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stemming\n",
    "class Stemmer:\n",
    "    '''\n",
    "    Convert uppercase to lowercase\n",
    "    Delete common endings of words\n",
    "    Can't delete white space and \\n(new line character)\n",
    "    Can't deal with Lemmatization. Ex. working => work\n",
    "    In generally, snowball is better than porter. So snowball method will be default.\n",
    "    '''\n",
    "    def __init__(self, stemmer_type='snowball'):\n",
    "        self.stemmer_type = stemmer_type\n",
    "        if self.stemmer_type == 'porter':\n",
    "            self.stemmer = nltk.stem.PorterStemmer()\n",
    "        elif self.stemmer_type == 'snowball':\n",
    "            self.stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "    def transform(self, text):\n",
    "        tokens = [self.stemmer.stem(token) for token in text.split(\" \")]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = '''Good muffin cost $ 3.88 in New York. Please buy me two of them. Thanks . \n",
    "#           I worked from home last \\n year when COVID-19 disaster.\n",
    "#           I am working.\n",
    "#           '''\n",
    "# Stemmer().transform(text) == 'good muffin cost $ 3.88 in new york. pleas buy me two of them. thank . \\n          i work from home last \\n year when covid-19 disaster.\\n          i am working.\\n          '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Stemmer():\n",
    "    text = '''Good muffin cost $ 3.88 in New York. Please buy me two of them. Thanks . \n",
    "          I worked from home last \\n year when COVID-19 disaster.\n",
    "          I am working.\n",
    "          '''\n",
    "    Stemmer().transform(text) == 'good muffin cost $ 3.88 in new york. pleas buy me two of them. thank . \\n          i work from home last \\n year when covid-19 disaster.\\n          i am working.\\n          '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* isinstance()<br>\n",
    "|函數|描述|\n",
    "|:----:|:----:|\n",
    "|isinstance(object, classinfo)    |    判斷 object 是否為 classinfo(類別) 的實體|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessorWrapper:\n",
    "    '''\n",
    "    help function input convert to string\n",
    "    '''\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def transform(self, input):\n",
    "        if isinstance(input, str): # check input whether is an str class instance\n",
    "            out = self.processor.transform(input)\n",
    "        elif isinstance(input, float) or isinstance(input, int):\n",
    "            out = self.processor.transform(str(input))\n",
    "        elif isinstance(input, list):\n",
    "            # take care when the input is a list\n",
    "            # currently for a list of attributes\n",
    "            out = [0]*len(input)\n",
    "            for i in range(len(input)):\n",
    "                out[i] = ProcessorWrapper(self.processor).transform(input[i])\n",
    "        else:\n",
    "            raise(ValueError(f'Currently not support type: {type(input).__name__}'))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s='lolololoooooo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(\n",
    "#     ProcessorWrapper(processor=Stemmer()).transform(s),\n",
    "#     ProcessorWrapper(processor=Stemmer()).transform(3.14159265358),\n",
    "#     ProcessorWrapper(processor=Stemmer()).transform(['a','b','b','c', 4, 5, 6.589]),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ProcessorWrapper():\n",
    "    s='lolololoooooo'\n",
    "    assert ProcessorWrapper(processor=Stemmer()).transform(s) == 'lolololoooooo'\n",
    "    assert ProcessorWrapper(processor=Stemmer()).transform(3.14159265358) == '3.14159265358'\n",
    "    assert ProcessorWrapper(processor=Stemmer()).transform(['a','b','b','c', 4, 5, 6.589]) == ['a', 'b', 'b', 'c', '4', '5', '6.589']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListProcessor:\n",
    "    \"\"\"\n",
    "    WARNING: This class will operate on the original input list itself\n",
    "    \"\"\"\n",
    "    def __init__(self, processors):\n",
    "        self.processors = processors\n",
    "\n",
    "    def process(self, lst):\n",
    "        for i in range(len(lst)):\n",
    "            for processor in self.processors:\n",
    "                lst[i] = ProcessorWrapper(processor).transform(lst[i])\n",
    "        return lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ListProcessor():\n",
    "    processors = [Lemmatizer(), Stemmer()]\n",
    "    lst = ['Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\nThanks.', 'Eric Tsai want a under cabinet.']\n",
    "    assert ListProcessor(processors).process(lst) == ['good muffin cost $ 3.88 in new york. pleas buy me two of them. thank .',\n",
    " 'eric tsai want a under cabinet .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameProcessor:\n",
    "    \"\"\"\n",
    "    WARNING: This class will operate on the original input dataframe itself\n",
    "    \"\"\"\n",
    "    def __init__(self, processors):\n",
    "        self.processors = processors\n",
    "\n",
    "    def process(self, series):  # I change the variable name: df-->series, because it is more clearly.\n",
    "        for processor in self.processors:\n",
    "            series = series.apply(ProcessorWrapper(processor).transform)\n",
    "        return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processors = [Lemmatizer(), Stemmer()]\n",
    "# dic = {'name': ['Eric', 'Ben'], \n",
    "#        'age': ['27', '58'],\n",
    "#        'education': ['National Chengchi University', 'Northwestern University']}\n",
    "# df = pd.DataFrame(data = dic)\n",
    "\n",
    "# DataFrameProcessor(processors).process(df.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_DataFrameProcessor():\n",
    "    processors = [Lemmatizer(), Stemmer()]\n",
    "    dic = {'name': ['Eric', 'Ben'], \n",
    "           'age': ['27', '58'],\n",
    "           'education': ['National Chengchi University', 'Northwestern University']}\n",
    "    df = pd.DataFrame(data = dic)\n",
    "    for i in range(len(df.name)):\n",
    "        assert DataFrameProcessor(processors).process(df.name)[i] == pd.Series(['eric', 'ben'], name='name')[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">***Note: This class is parallel function. It can't operate in notebook directly. So I don't want to write unit test here***</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 多進程 (multiprocessing) 必須用 terminal 才可順利運行，或是建一個腳本檔，\n",
    "## 將所有相關的 function 建置其中，之後再import進 notebook \n",
    "class DataFrameParallelProcessor:\n",
    "    \"\"\"\n",
    "    WARNING: This class will operate on the original input dataframe itself\n",
    "\n",
    "    https://stackoverflow.com/questions/26520781/multiprocessing-pool-whats-the-difference-between-map-async-and-imap\n",
    "    \"\"\"\n",
    "    def __init__(self, processors, n_jobs=1):  # my notebook only has 2 CPU\n",
    "        self.processors = processors\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def process(self, dfAll, columns):\n",
    "        df_processor = DataFrameProcessor(self.processors)\n",
    "        p = multiprocessing.Pool(self.n_jobs)\n",
    "        dfs = p.imap(df_processor.process, [dfAll[col] for col in columns])\n",
    "        for col,df in zip(columns, dfs):\n",
    "            dfAll[col] = df\n",
    "        return dfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = {'name': ['Eric', 'Ben'], \n",
    "#    'age': ['27', '58'],\n",
    "#    'education': ['National Chengchi University', 'Northwestern University']}\n",
    "# df = pd.DataFrame(data = dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processors = [pp.Lemmatizer(), pp.Stemmer()]\n",
    "# test_result = pp. DataFrameParallelProcessor(processors).process(df, columns = ['name', 'education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df,test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------- Query Expansion -------------------\n",
    "# 3rd solution in CrowdFlower (Chenglong team decided to remove the feature which might be a major cause of overfitting.)\n",
    "class QueryExpansion():\n",
    "    '''\n",
    "    if stopwords_threshold decrease, the number of stop word will raise\n",
    "    '''\n",
    "    def __init__(self, df, ngram=3, stopwords_threshold=0.9, base_stopwords=set()):  # base_stopwords: can add some stop word at the start\n",
    "        self.df = df[[\"search_term\", \"product_title\"]].copy()\n",
    "        self.ngram = ngram\n",
    "        self.stopwords_threshold = stopwords_threshold\n",
    "        self.stopwords = set(base_stopwords).union(self._get_customized_stopwords())\n",
    "        \n",
    "    def _get_customized_stopwords(self):\n",
    "        '''\n",
    "        get stopwords with low frequency\n",
    "        '''\n",
    "        words = \" \".join(list(self.df[\"product_title\"].values)).split(\" \")\n",
    "        # find word frequency\n",
    "        counter = Counter(words)\n",
    "        # find unique word\n",
    "        num_uniq = len(list(counter.keys()))\n",
    "        # define the amount of stop word (if a word frequency is too high, then it will be a stopword)\n",
    "        num_stop = int((1.-self.stopwords_threshold)*num_uniq)\n",
    "        stopwords = set()\n",
    "        for e,(w,c) in enumerate(sorted(counter.items(), key=lambda x: x[1])):\n",
    "            if e == num_stop:\n",
    "                break\n",
    "            stopwords.add(w)\n",
    "        return stopwords\n",
    "\n",
    "    def _ngram(self, text):\n",
    "        tokens = text.split(\" \")\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]\n",
    "        return ngram_utils._ngrams(tokens, self.ngram, \" \")\n",
    "\n",
    "    def _get_alternative_query(self, lst):\n",
    "        ''' \n",
    "        Through product describtion to find the most frequency word in ngram list \n",
    "        '''\n",
    "        res = []\n",
    "        for v in lst:\n",
    "            res += v\n",
    "        c = Counter(res)\n",
    "        value, count = c.most_common()[0]\n",
    "        return value\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Through the above function '_get_alternative_query' to set the data process\n",
    "        '''\n",
    "        self.df[\"title_ngram\"] = self.df[\"product_title\"].apply(self._ngram)\n",
    "        corpus = self.df.groupby(\"search_term\").apply(lambda x: self._get_alternative_query(x[\"title_ngram\"]))\n",
    "        corpus = corpus.reset_index()\n",
    "        corpus.columns = [\"search_term\", \"search_term_alt\"]\n",
    "        self.df = pd.merge(self.df, corpus, on=\"search_term\", how=\"left\")\n",
    "        return self.df[\"search_term_alt\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Testing: QueryExpansion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### define parameter we need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create data frame\n",
    "# dic = {'search_term': ['Eric', 'Ben', 'Eric'], \n",
    "#    'age': ['27', '58', '66'],\n",
    "#    'product_title': ['a a a a b b b b c c c c d d d d e e e e f f f f g g g g h h h h i i i i j j j j k k k k l l l l mostA mostA mostA mostA mostA mostA',\n",
    "#                      'a a a a b b b b c c c c d d d d e e e e f f f f g g g g h h h h i i i i j j j j k k k k l l l l mostB mostB mostB mostB mostB mostB',\n",
    "#                      'stopword_1 stopword_2 stopword_3 stopword_4 stopword_5']}\n",
    "# df = pd.DataFrame(data = dic)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### auto create a stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = QueryExpansion(df)._get_customized_stopwords()\n",
    "# stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### auto create alternative search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['title_ngram'] = df['product_title'].apply(QueryExpansion(df)._ngram)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = df.groupby('search_term').apply(lambda df: QueryExpansion(df)._get_alternative_query(df['title_ngram']))\n",
    "# corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### return the array which is alternative query for search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QueryExpansion(df).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_QueryExpansion():\n",
    "    # create data frame\n",
    "    dic = {'search_term': ['Eric', 'Ben', 'Eric'], \n",
    "       'age': ['27', '58', '66'],\n",
    "       'product_title': ['a a a a b b b b c c c c d d d d e e e e f f f f g g g g h h h h i i i i j j j j k k k k l l l l mostA mostA mostA mostA mostA mostA',\n",
    "                         'a a a a b b b b c c c c d d d d e e e e f f f f g g g g h h h h i i i i j j j j k k k k l l l l mostB mostB mostB mostB mostB mostB',\n",
    "                         'stopword_1 stopword_2 stopword_3 stopword_4 stopword_5']}\n",
    "    df_1 = pd.DataFrame(data = dic)\n",
    "    assert QueryExpansion(df_1)._get_customized_stopwords() == {'stopword_1'}\n",
    "    \n",
    "    assert QueryExpansion(df_1)._ngram('a b c d e f g h') == ['a b c', 'b c d', 'c d e', 'd e f', 'e f g', 'f g h']\n",
    "    \n",
    "    '''\n",
    "    assume text = aaaaabc\n",
    "    n=3\n",
    "    ngram_list = ['a a a', 'a a a', 'a a a', 'a a b', 'a b c']\n",
    "    '''\n",
    "    # function: '_get_alternative_query' will group by search term, so I set the search term be the same here\n",
    "    dic = {'search_term': ['Eric', 'Eric', 'Eric'], \n",
    "           'product_title': ['aaaaabc',\n",
    "                             'bbbbbac',\n",
    "                             'aaaaabc']}\n",
    "    df_2 = pd.DataFrame(data = dic)\n",
    "    # we can find out the compound word(or string) 'a a a' has the most frequency here\n",
    "    ngram_series = pd.Series([['a a a', 'a a a', 'a a a', 'a a b', 'a b c'],\n",
    "                              ['b b b', 'b b b', 'b b b', 'b b a', 'b a c'],\n",
    "                              ['a a a', 'a a a', 'a a a', 'a a b', 'a b c']])\n",
    "    # so excepted output is 'a a a'\n",
    "    assert QueryExpansion(df_2)._get_alternative_query(ngram_series) == 'a a a'\n",
    "    \n",
    "    assert (QueryExpansion(df_1).build()[0] \n",
    "            == np.array(['mostA mostA mostA', 'mostB mostB mostB', 'mostA mostA mostA'], dtype='object'))[0]\n",
    "    \n",
    "    assert (QueryExpansion(df_1).build()[1]\n",
    "            == np.array(['mostA mostA mostA', 'mostB mostB mostB', 'mostA mostA mostA'], dtype='object'))[1]\n",
    "    \n",
    "    assert (QueryExpansion(df_1).build()[2]\n",
    "            == np.array(['mostA mostA mostA', 'mostB mostB mostB', 'mostA mostA mostA'], dtype='object'))[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Product Name\n",
    "1. Find color string, using homemade pattern.\n",
    "2. Find units string, using UnitConverter(), I create before.\n",
    "3. Find the other pattern, which is not product name and remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------- Extract Product Name -------------------\n",
    "# 3rd solution in CrowdFlower\n",
    "color_data = SourceFileLoader('COLOR_LIST', config.COLOR_DATA).load_module()  # import specific .py file which is not has same path with this file\n",
    "COLORS_PATTERN = r'(?<=\\W|^)%s(?=\\W|$)'%('|'.join(color_data.COLOR_LIST))\n",
    "UNITS = [' '.join(r.strip().split(' ')[1:]) for p,r in UnitConverter().pattern_replace_pair_list]\n",
    "UNITS_PATTERN = r'(?:\\d+[?:.,]?\\d*)(?: %s\\.*)?'%('|'.join(UNITS))\n",
    "DIM_PATTERN_NxNxN = r'%s ?x %s ?x %s'%(UNITS_PATTERN, UNITS_PATTERN, UNITS_PATTERN)\n",
    "DIM_PATTERN_NxN = r'%s ?x %s'%(UNITS_PATTERN, UNITS_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_data = imp.load_source('', config.COLOR_DATA)\n",
    "# COLORS_PATTERN = r'(?<=\\W|^)%s(?=\\W|$)'%('|'.join(color_data.COLOR_LIST))\n",
    "# COLORS_PATTERN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # p: original pattern  r: replace pattern\n",
    "# # strip(): remove head and tail white spaces\n",
    "# # find all unit which set before by UnitConverter().pattern_replace_pair_list\n",
    "# UNITS = [' '.join(r.strip().split(' ')[1:]) for p,r in UnitConverter().pattern_replace_pair_list]\n",
    "# UNITS_PATTERN = r'(?:\\d+[?:.,]?\\d*)(?: %s\\.*)?'%('|'.join(UNITS))\n",
    "# display(UNITS, UNITS_PATTERN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIM_PATTERN_NxNxN = r'%s ?x %s ?x %s'%(UNITS_PATTERN, UNITS_PATTERN, UNITS_PATTERN)\n",
    "# DIM_PATTERN_NxN = r'%s ?x %s'%(UNITS_PATTERN, UNITS_PATTERN)\n",
    "# display(DIM_PATTERN_NxNxN)\n",
    "# print('===================================================================================================')\n",
    "# display(DIM_PATTERN_NxN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">***In original version, code is not correct. I think the problem is 3rd solution in CrowdFlower don't use 'regex' modual (only use 're' modual). So, in this case, we need to add 'r' in front of string, and replace `\\\\b` to `\\b`. However, this task is not specifically designed for this case. But it is helpful.***</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\b: 匹配一個詞的邊界。一個詞的邊界就是一個詞不被另外一個“字”字符跟隨的位置或者前面跟其他“字”字符的位置，例如在字母和空格之間。注意，匹配中不包括匹配的字邊界。換句話說，一個匹配的詞的邊界的內容的長度是0\n",
    "```python\n",
    "text = '1234 5687 1918711'\n",
    "# match '1', but 1 must be back boundary\n",
    "regex.sub('1\\b','', text) = '1234 5687 191871'\n",
    "# match '1', but 1 must be Front boundary\n",
    "regex.sub('\\b1','', text) = '234 5687 918711'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd solution in CrowdFlower\n",
    "class ProductNameExtractor(BaseReplacer):\n",
    "    def __init__(self):\n",
    "        self.pattern_replace_pair_list = [\n",
    "            # Remove descriptions (text between paranthesis'()'/brackets'[]')\n",
    "            (r'[ ]?[[(].+?[])]', r''),\n",
    "            # Remove 'made in...'\n",
    "            (r'made in [a-z]+\\b', r''),\n",
    "            # Remove descriptions (hyphen'-' or comma',' followed by space then at most 2 words, repeated)\n",
    "            (r'([,-]( ([a-zA-Z0-9]+\\b)){1,2}[ ]?){1,}$', r''),\n",
    "            # Remove descriptions (prepositions staring with: with, for, by, in )\n",
    "            (r'\\b(with|for|by|in|w/) .+$', r''),\n",
    "            # colors & sizes\n",
    "            (r'size: .+$', r''),\n",
    "            (r'size [0-9]+[.]?[0-9]+\\b', r''),\n",
    "            (COLORS_PATTERN, r''),\n",
    "            # dimensions\n",
    "            (DIM_PATTERN_NxNxN, r''),\n",
    "            (DIM_PATTERN_NxN, r''),\n",
    "            # measurement units\n",
    "            (UNITS_PATTERN, r''),\n",
    "            # others\n",
    "            (r'(value bundle|warranty|brand new|excellent condition|one size|new in box|authentic|as is)', r''),\n",
    "            # stop words\n",
    "            (r'\\b(in)\\b', r''),\n",
    "            # hyphenated words\n",
    "            (r'([a-zA-Z])-([a-zA-Z])', r'\\1\\2'),\n",
    "            # special characters\n",
    "            (r'[ &<>)(_,.;:!?/+#*-]+', r' '),\n",
    "            # numbers that are not part of a word\n",
    "            (r'\\b[0-9]+\\b', r''),\n",
    "        ]\n",
    "        \n",
    "    def preprocess(self, text):\n",
    "        pattern_replace_pair_list = [\n",
    "            # Remove single & double apostrophes\n",
    "            (r'[\\']+', r''),\n",
    "            # Remove product codes (long words (>5 characters) that are all caps, numbers or mix pf both)\n",
    "            # don't use raw string format\n",
    "            (r'[ ]?\\b[0-9A-Z-]{5,}\\b', r''),\n",
    "        ]\n",
    "        text = BaseReplacer(pattern_replace_pair_list).transform(text)\n",
    "        text = LowerCaseConverter().transform(text)\n",
    "        text = DigitLetterSplitter().transform(text)\n",
    "        text = UnitConverter().transform(text)\n",
    "        text = DigitCommaDigitMerger().transform(text)\n",
    "        text = NumberDigitMapper().transform(text)\n",
    "        text = UnitConverter().transform(text)\n",
    "        return text\n",
    "        \n",
    "    def transform(self, text):\n",
    "        text = super().transform(self.preprocess(text))\n",
    "        text = Lemmatizer().transform(text)\n",
    "        text = Stemmer(stemmer_type='snowball').transform(text)\n",
    "        # last two words in product\n",
    "        text = ' '.join(text.split(' ')[-2:])\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------- Process Attributes -------------------\n",
    "def _split_attr_to_text(text):\n",
    "    attrs = text.split(config.ATTR_SEPARATOR)  # ' | '\n",
    "    return ' '.join(attrs)\n",
    "\n",
    "def _split_attr_to_list(text):\n",
    "    attrs = text.split(config.ATTR_SEPARATOR)        \n",
    "    if len(attrs) == 1:\n",
    "        # missing\n",
    "        return [[attrs[0], attrs[0]]]\n",
    "    else:  # attrs[::2]: means return values which are according to indexes order 0,2,4,6,... \n",
    "        return [[n,v] for n,v in zip(attrs[::2], attrs[1::2])]  # attrs[1::2]: means return values which are according to indexes order 1,3,5,... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_attr_to_text():\n",
    "    text = 'A | A_attr | B | B_attr | C | C_attr'\n",
    "    assert _split_attr_to_text(text) == 'A A_attr B B_attr C C_attr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_attr_to_list():\n",
    "    text = 'A | A_attr | B | B_attr | C | C_attr'\n",
    "    assert _split_attr_to_list(text) == [['A', 'A_attr'], ['B', 'B_attr'], ['C', 'C_attr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll = pkl_utils._load(config.ALL_DATA_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = dfAll['product_attribute_concat'][0]\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _split_attr_to_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _split_attr_to_list(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Record Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-31-18-11'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = time_utils._timestamp()\n",
    "now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## Setup ##\n",
    "###########\n",
    "logname = f'data_processor_{now}.log'\n",
    "logger = logging_utils._get_logger(config.LOG_DIR, logname)\n",
    "\n",
    "\n",
    "# Put product_attribute_list, product_attribute and product_description first as they are\n",
    "# quite time consuming to process.\n",
    "# Choose the columns by check data_preparer.ipynb. In the end, the notebook will show the clean data frame.\n",
    "columns_to_proc = [\n",
    "    # # product_attribute_list is very time consuming to process\n",
    "    # # so we just process product_attribute which is of the form \n",
    "    # # attr_name1 | attr_value1 | attr_name2 | attr_value2 | ...\n",
    "    # # and split it into a list afterwards\n",
    "    # 'product_attribute_list',\n",
    "    'product_attribute_concat',\n",
    "    'product_description',\n",
    "    'product_brand', \n",
    "    'product_color',\n",
    "    'product_title',\n",
    "    'search_term', \n",
    "]\n",
    "if config.PLATFORM == 'Linux':\n",
    "    config.DATA_PROCESSOR_N_JOBS = len(columns_to_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "1/2 inch rubber lep tips Bullet07\n",
      "After:\n",
      "['1/2 in. rubber lep tip bullet 07']\n"
     ]
    }
   ],
   "source": [
    "# clean using a list of processors\n",
    "processors = [\n",
    "    LowerCaseConverter(), \n",
    "    # See LowerUpperCaseSplitter and UnitConverter for why we put UnitConverter here\n",
    "    # 其實沒差，除非能處理掉數字加介係詞 in 的狀況不被替代成單位 in.(inch)\n",
    "    UnitConverter(),\n",
    "    LowerUpperCaseSplitter(), \n",
    "    WordReplacer(replace_fname=config.WORD_REPLACER_DATA), \n",
    "    LetterLetterSplitter(),\n",
    "    DigitLetterSplitter(), \n",
    "    DigitCommaDigitMerger(), \n",
    "    NumberDigitMapper(),\n",
    "    UnitConverter(), \n",
    "    QuartetCleaner(), \n",
    "    HtmlCleaner(parser='html.parser'), \n",
    "    Lemmatizer(),\n",
    "]\n",
    "stemmers = [\n",
    "    Stemmer(stemmer_type='snowball'), \n",
    "    Stemmer(stemmer_type='porter')\n",
    "][0:1]  # means only use Stemmer(stemmer_type='snowball')\n",
    "\n",
    "## simple test\n",
    "text = '1/2 inch rubber lep tips Bullet07'\n",
    "print('Original:')\n",
    "print(text)\n",
    "list_processor = ListProcessor(processors)\n",
    "print('After:')\n",
    "print(list_processor.process([text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "## Process ##\n",
    "#############\n",
    "## load raw data\n",
    "dfAll = pkl_utils._load(config.ALL_DATA_RAW)\n",
    "columns_to_proc = [col for col in columns_to_proc if col in dfAll.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 200\n"
     ]
    }
   ],
   "source": [
    "if config.TASK == 'sample':\n",
    "    dfAll = dfAll.iloc[0:config.SAMPLE_SIZE]\n",
    "    print(f'data length: {len(dfAll)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Product Name from `search_term` and `product_title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 search_term search_term_product_name  \\\n",
      "0                              angle bracket             angl bracket   \n",
      "1                                  l bracket                l bracket   \n",
      "2                                  deck over                deck over   \n",
      "3                           rain shower head              shower head   \n",
      "4                         shower only faucet              onli faucet   \n",
      "..                                       ...                      ...   \n",
      "195  rigid lithium ion batteries fuego drill              fuego drill   \n",
      "196                                  chipper                  chipper   \n",
      "197                                    bidet                    bidet   \n",
      "198                            shark cleaner            shark cleaner   \n",
      "199                             shark vacuum             shark vacuum   \n",
      "\n",
      "    product_title_product_name  \n",
      "0                    gaug angl  \n",
      "1                    gaug angl  \n",
      "2                 concret coat  \n",
      "3                     trim kit  \n",
      "4                     trim kit  \n",
      "..                         ...  \n",
      "195                  combo kit  \n",
      "196              mulcher shder  \n",
      "197                  bowl onli  \n",
      "198             vacuum cleaner  \n",
      "199             vacuum cleaner  \n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "## extract product name from search_term and product_title\n",
    "ext = ProductNameExtractor()\n",
    "dfAll['search_term_product_name'] = dfAll['search_term'].apply(ext.transform)\n",
    "dfAll['product_title_product_name'] = dfAll['product_title'].apply(ext.transform)\n",
    "if config.TASK == 'sample':\n",
    "    print(dfAll[['search_term', 'search_term_product_name', 'product_title_product_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.GOOGLE_CORRECTING_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean using GoogleQuerySpellingChecker(Chenglong team not used in final submission)\n",
    "# MUST BE IN FRONT OF ALL THE PROCESSING\n",
    "if config.GOOGLE_CORRECTING_QUERY:\n",
    "    logger.info('Run GoogleQuerySpellingChecker at search_term')\n",
    "    checker = GoogleQuerySpellingChecker()\n",
    "    dfAll['search_term'] = dfAll['search_term'].apply(checker.correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     product_attribute  \\\n",
      "0    bullet 01 versatile connector for various 90 d...   \n",
      "1    bullet 01 versatile connector for various 90 d...   \n",
      "2    application method brush roller spray assemble...   \n",
      "3    bath faucet type combo tub and shower built in...   \n",
      "4    bath faucet type combo tub and shower built in...   \n",
      "..                                                 ...   \n",
      "195  battery included yes battery amp hour 1.5 batt...   \n",
      "196  amperage amp 13 bag maximum load capacity bush...   \n",
      "197  bowl height in. 17 bowl shape elongated bullet...   \n",
      "198                                       missingvalue   \n",
      "199                                       missingvalue   \n",
      "\n",
      "                                product_attribute_list  \n",
      "0    [[bullet 01, versatile connector for various 9...  \n",
      "1    [[bullet 01, versatile connector for various 9...  \n",
      "2    [[application method, brush roller spray], [as...  \n",
      "3    [[bath faucet type, combo tub and shower], [bu...  \n",
      "4    [[bath faucet type, combo tub and shower], [bu...  \n",
      "..                                                 ...  \n",
      "195  [[battery included, yes], [battery amp hour, 1...  \n",
      "196  [[amperage amp, 13], [bag maximum load capacit...  \n",
      "197  [[bowl height in., 17], [bowl shape, elongated...  \n",
      "198                     [[missingvalue, missingvalue]]  \n",
      "199                     [[missingvalue, missingvalue]]  \n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "## clean uisng a list of processors\n",
    "df_processor = pp.DataFrameParallelProcessor(pp.processors, config.DATA_PROCESSOR_N_JOBS)\n",
    "df_processor.process(dfAll, columns_to_proc)\n",
    "# split product_attribute_concat into product_attribute and product_attribute_list\n",
    "dfAll['product_attribute'] = dfAll['product_attribute_concat'].apply(_split_attr_to_text)\n",
    "dfAll['product_attribute_list'] = dfAll['product_attribute_concat'].apply(_split_attr_to_list)\n",
    "if config.TASK == 'sample':\n",
    "    print(dfAll[['product_attribute', 'product_attribute_list']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll[['product_attribute', 'product_attribute_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.QUERY_EXPANSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query expansion (Chenglong team decided to remove the feature which might be a major cause of overfitting.)\n",
    "if config.QUERY_EXPANSION:\n",
    "    list_processor = ListProcessor(processors)\n",
    "    # stop words must to access data process. EX. NumberDigitMapper function will replace 'one' to '1'.\n",
    "    # So, if stop word has 'one', it must replace to '1',too. \n",
    "    base_stopwords = set(list_processor.process(list(config.STOP_WORDS)))  # a set of stop word\n",
    "    qe = QueryExpansion(dfAll, ngram=3, stopwords_threshold=0.9, base_stopwords=base_stopwords)\n",
    "    dfAll['search_term_alt'] = qe.build()\n",
    "    if config.TASK == 'sample':\n",
    "        print(dfAll[['search_term', 'search_term_alt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "logger.info(f'Save to {config.ALL_DATA_LEMMATIZED}')\n",
    "columns_to_save = [col for col in dfAll.columns if col != 'product_attribute_concat']\n",
    "pkl_utils._save(config.ALL_DATA_LEMMATIZED, dfAll[columns_to_save])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.AUTO_CORRECTING_QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## auto correcting query(Chenglong team not used in final submission)\n",
    "if config.AUTO_CORRECTING_QUERY:\n",
    "    logger.info('Run AutoSpellingChecker at search_term')\n",
    "    checker = AutoSpellingChecker(dfAll, exclude_stopwords=False, min_len=4)\n",
    "    dfAll['search_term_auto_corrected'] = list(dfAll['search_term'].apply(checker.correct))\n",
    "    columns_to_proc += ['search_term_auto_corrected']\n",
    "    if config.TASK == 'sample':\n",
    "        print(dfAll[['search_term', 'search_term_auto_corrected']])\n",
    "    # save query_correction_map and spelling checker\n",
    "    fname = '%s/auto_spelling_checker_query_correction_map_%s.log'%(config.LOG_DIR, now)\n",
    "    checker.save_query_correction_map(fname)\n",
    "    # save data\n",
    "    logger.info('Save to %s'%config.ALL_DATA_LEMMATIZED)\n",
    "    columns_to_save = [col for col in dfAll.columns if col != 'product_attribute_concat']\n",
    "    pkl_utils._save(config.ALL_DATA_LEMMATIZED, dfAll[columns_to_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean using stemmers\n",
    "df_processor = pp.DataFrameParallelProcessor(pp.stemmers, config.DATA_PROCESSOR_N_JOBS)\n",
    "df_processor.process(dfAll, columns_to_proc)\n",
    "# split product_attribute_concat into product_attribute and product_attribute_list\n",
    "dfAll['product_attribute'] = dfAll['product_attribute_concat'].apply(_split_attr_to_text)\n",
    "dfAll['product_attribute_list'] = dfAll['product_attribute_concat'].apply(_split_attr_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.QUERY_EXPANSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query expansion\n",
    "if config.QUERY_EXPANSION:\n",
    "    list_processor = ListProcessor(stemmers)\n",
    "    base_stopwords = set(list_processor.process(list(config.STOP_WORDS)))\n",
    "    qe = QueryExpansion(dfAll, ngram=3, stopwords_threshold=0.9, base_stopwords=base_stopwords)\n",
    "    dfAll['search_term_alt'] = qe.build()\n",
    "    if config.TASK == 'sample':\n",
    "        print(dfAll[['search_term', 'search_term_alt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "logger.info('Save to %s'%config.ALL_DATA_LEMMATIZED_STEMMED)\n",
    "columns_to_save = [col for col in dfAll.columns if col != 'product_attribute_concat']\n",
    "pkl_utils._save(config.ALL_DATA_LEMMATIZED_STEMMED, dfAll[columns_to_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts =============================\n",
      "platform win32 -- Python 3.7.7, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- D:\\Anaconda3\\envs\\HomeDepotProductSearchRelevance\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: D:\\python\\kaggle_compete\\Home_Depot_Product_Search_Relevance\\Code\n",
      "plugins: dash-1.14.0\n",
      "collecting ... collected 19 items\n",
      "\n",
      "data_processor.py::test_BaseReplacer PASSED                              [  5%]\n",
      "data_processor.py::test_LowerCaseConverter PASSED                        [ 10%]\n",
      "data_processor.py::test_LowerUpperCaseSplitter PASSED                    [ 15%]\n",
      "data_processor.py::test_WordReplacer PASSED                              [ 21%]\n",
      "data_processor.py::test_LetterLetterSplitter PASSED                      [ 26%]\n",
      "data_processor.py::test_DigitLetterSplitter PASSED                       [ 31%]\n",
      "data_processor.py::test_DigitCommaDigitMerger PASSED                     [ 36%]\n",
      "data_processor.py::test_NumberDigitMapper PASSED                         [ 42%]\n",
      "data_processor.py::test_UnitConverter PASSED                             [ 47%]\n",
      "data_processor.py::test_HtmlCleaner PASSED                               [ 52%]\n",
      "data_processor.py::test_QuartetCleaner PASSED                            [ 57%]\n",
      "data_processor.py::test_Lemmatizer PASSED                                [ 63%]\n",
      "data_processor.py::test_Stemmer PASSED                                   [ 68%]\n",
      "data_processor.py::test_ProcessorWrapper PASSED                          [ 73%]\n",
      "data_processor.py::test_ListProcessor PASSED                             [ 78%]\n",
      "data_processor.py::test_DataFrameProcessor PASSED                        [ 84%]\n",
      "data_processor.py::test_QueryExpansion PASSED                            [ 89%]\n",
      "data_processor.py::test_split_attr_to_text PASSED                        [ 94%]\n",
      "data_processor.py::test_split_attr_to_list PASSED                        [100%]\n",
      "\n",
      "======================= 19 passed in 208.83s (0:03:28) ========================\n"
     ]
    }
   ],
   "source": [
    "!py.test -vv data_processor.py -Wignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Modular Design\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "1/2 inch rubber lep tips Bullet07\n",
      "After:\n",
      "['1/2 in. rubber lep tip bullet 07']\n",
      "data length: 200\n",
      "                                 search_term search_term_product_name  \\\n",
      "0                              angle bracket             angl bracket   \n",
      "1                                  l bracket                l bracket   \n",
      "2                                  deck over                deck over   \n",
      "3                           rain shower head              shower head   \n",
      "4                         shower only faucet              onli faucet   \n",
      "..                                       ...                      ...   \n",
      "195  rigid lithium ion batteries fuego drill              fuego drill   \n",
      "196                                  chipper                  chipper   \n",
      "197                                    bidet                    bidet   \n",
      "198                            shark cleaner            shark cleaner   \n",
      "199                             shark vacuum             shark vacuum   \n",
      "\n",
      "    product_title_product_name  \n",
      "0                    gaug angl  \n",
      "1                    gaug angl  \n",
      "2                 concret coat  \n",
      "3                     trim kit  \n",
      "4                     trim kit  \n",
      "..                         ...  \n",
      "195                  combo kit  \n",
      "196              mulcher shder  \n",
      "197                  bowl onli  \n",
      "198             vacuum cleaner  \n",
      "199             vacuum cleaner  \n",
      "\n",
      "[200 rows x 3 columns]\n",
      "                                     product_attribute  \\\n",
      "0    bullet 01 versatile connector for various 90 d...   \n",
      "1    bullet 01 versatile connector for various 90 d...   \n",
      "2    application method brush roller spray assemble...   \n",
      "3    bath faucet type combo tub and shower built in...   \n",
      "4    bath faucet type combo tub and shower built in...   \n",
      "..                                                 ...   \n",
      "195  battery included yes battery amp hour 1.5 batt...   \n",
      "196  amperage amp 13 bag maximum load capacity bush...   \n",
      "197  bowl height in. 17 bowl shape elongated bullet...   \n",
      "198                                       missingvalue   \n",
      "199                                       missingvalue   \n",
      "\n",
      "                                product_attribute_list  \n",
      "0    [[bullet 01, versatile connector for various 9...  \n",
      "1    [[bullet 01, versatile connector for various 9...  \n",
      "2    [[application method, brush roller spray], [as...  \n",
      "3    [[bath faucet type, combo tub and shower], [bu...  \n",
      "4    [[bath faucet type, combo tub and shower], [bu...  \n",
      "..                                                 ...  \n",
      "195  [[battery included, yes], [battery amp hour, 1...  \n",
      "196  [[amperage amp, 13], [bag maximum load capacit...  \n",
      "197  [[bowl height in., 17], [bowl shape, elongated...  \n",
      "198                     [[missingvalue, missingvalue]]  \n",
      "199                     [[missingvalue, missingvalue]]  \n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ### 1. Record Time\n",
    "    now = time_utils._timestamp()\n",
    "    ###########\n",
    "    ## Setup ##\n",
    "    ###########\n",
    "    logname = f'data_processor_{now}.log'\n",
    "    logger = logging_utils._get_logger(config.LOG_DIR, logname)\n",
    "\n",
    "\n",
    "    # Put product_attribute_list, product_attribute and product_description first as they are\n",
    "    # quite time consuming to process.\n",
    "    # Choose the columns by check data_preparer.ipynb. In the end, the notebook will show the clean data frame.\n",
    "    columns_to_proc = [\n",
    "        # # product_attribute_list is very time consuming to process\n",
    "        # # so we just process product_attribute which is of the form \n",
    "        # # attr_name1 | attr_value1 | attr_name2 | attr_value2 | ...\n",
    "        # # and split it into a list afterwards\n",
    "        # 'product_attribute_list',\n",
    "        'product_attribute_concat',\n",
    "        'product_description',\n",
    "        'product_brand', \n",
    "        'product_color',\n",
    "        'product_title',\n",
    "        'search_term', \n",
    "    ]\n",
    "    if config.PLATFORM == 'Linux':\n",
    "        config.DATA_PROCESSOR_N_JOBS = len(columns_to_proc)\n",
    "\n",
    "    # clean using a list of processors\n",
    "    processors = [\n",
    "        LowerCaseConverter(), \n",
    "        # See LowerUpperCaseSplitter and UnitConverter for why we put UnitConverter here\n",
    "        # 其實沒差，除非能處理掉數字加介係詞 in 的狀況不被替代成單位 in.(inch)\n",
    "        UnitConverter(),\n",
    "        LowerUpperCaseSplitter(), \n",
    "        WordReplacer(replace_fname=config.WORD_REPLACER_DATA), \n",
    "        LetterLetterSplitter(),\n",
    "        DigitLetterSplitter(), \n",
    "        DigitCommaDigitMerger(), \n",
    "        NumberDigitMapper(),\n",
    "        UnitConverter(), \n",
    "        QuartetCleaner(), \n",
    "        HtmlCleaner(parser='html.parser'), \n",
    "        Lemmatizer(),\n",
    "    ]\n",
    "    stemmers = [\n",
    "        Stemmer(stemmer_type='snowball'), \n",
    "        Stemmer(stemmer_type='porter')\n",
    "    ][0:1]  # means only use Stemmer(stemmer_type='snowball')\n",
    "\n",
    "    ## simple test\n",
    "    text = '1/2 inch rubber lep tips Bullet07'\n",
    "    print('Original:')\n",
    "    print(text)\n",
    "    list_processor = ListProcessor(processors)\n",
    "    print('After:')\n",
    "    print(list_processor.process([text]))\n",
    "\n",
    "    #############\n",
    "    ## Process ##\n",
    "    #############\n",
    "    ## load raw data\n",
    "    dfAll = pkl_utils._load(config.ALL_DATA_RAW)\n",
    "    columns_to_proc = [col for col in columns_to_proc if col in dfAll.columns]\n",
    "\n",
    "    if config.TASK == 'sample':\n",
    "        dfAll = dfAll.iloc[0:config.SAMPLE_SIZE]\n",
    "        print(f'data length: {len(dfAll)}')\n",
    "\n",
    "    ## extract product name from search_term and product_title\n",
    "    ext = ProductNameExtractor()\n",
    "    dfAll['search_term_product_name'] = dfAll['search_term'].apply(ext.transform)\n",
    "    dfAll['product_title_product_name'] = dfAll['product_title'].apply(ext.transform)\n",
    "    if config.TASK == 'sample':\n",
    "        print(dfAll[['search_term', 'search_term_product_name', 'product_title_product_name']])\n",
    "\n",
    "    ## clean using GoogleQuerySpellingChecker(Chenglong team not used in final submission)\n",
    "    # MUST BE IN FRONT OF ALL THE PROCESSING\n",
    "    if config.GOOGLE_CORRECTING_QUERY:\n",
    "        logger.info('Run GoogleQuerySpellingChecker at search_term')\n",
    "        checker = GoogleQuerySpellingChecker()\n",
    "        dfAll['search_term'] = dfAll['search_term'].apply(checker.correct)\n",
    "\n",
    "    ## clean uisng a list of processors\n",
    "    df_processor = pp.DataFrameParallelProcessor(pp.processors, config.DATA_PROCESSOR_N_JOBS)\n",
    "    df_processor.process(dfAll, columns_to_proc)\n",
    "    # split product_attribute_concat into product_attribute and product_attribute_list\n",
    "    dfAll['product_attribute'] = dfAll['product_attribute_concat'].apply(_split_attr_to_text)\n",
    "    dfAll['product_attribute_list'] = dfAll['product_attribute_concat'].apply(_split_attr_to_list)\n",
    "    if config.TASK == 'sample':\n",
    "        print(dfAll[['product_attribute', 'product_attribute_list']])\n",
    "\n",
    "\n",
    "    # query expansion (Chenglong team decided to remove the feature which might be a major cause of overfitting.)\n",
    "    if config.QUERY_EXPANSION:\n",
    "        list_processor = ListProcessor(processors)\n",
    "        # stop words must to access data process. EX. NumberDigitMapper function will replace 'one' to '1'.\n",
    "        # So, if stop word has 'one', it must replace to '1',too. \n",
    "        base_stopwords = set(list_processor.process(list(config.STOP_WORDS)))  # a set of stop word\n",
    "        qe = QueryExpansion(dfAll, ngram=3, stopwords_threshold=0.9, base_stopwords=base_stopwords)\n",
    "        dfAll['search_term_alt'] = qe.build()\n",
    "        if config.TASK == 'sample':\n",
    "            print(dfAll[['search_term', 'search_term_alt']])\n",
    "\n",
    "    # save data\n",
    "    logger.info(f'Save to {config.ALL_DATA_LEMMATIZED}')\n",
    "    columns_to_save = [col for col in dfAll.columns if col != 'product_attribute_concat']\n",
    "    pkl_utils._save(config.ALL_DATA_LEMMATIZED, dfAll[columns_to_save])\n",
    "\n",
    "\n",
    "    ## auto correcting query(Chenglong team not used in final submission)\n",
    "    if config.AUTO_CORRECTING_QUERY:\n",
    "        logger.info('Run AutoSpellingChecker at search_term')\n",
    "        checker = AutoSpellingChecker(dfAll, exclude_stopwords=False, min_len=4)\n",
    "        dfAll['search_term_auto_corrected'] = list(dfAll['search_term'].apply(checker.correct))\n",
    "        columns_to_proc += ['search_term_auto_corrected']\n",
    "        if config.TASK == 'sample':\n",
    "            print(dfAll[['search_term', 'search_term_auto_corrected']])\n",
    "        # save query_correction_map and spelling checker\n",
    "        fname = '%s/auto_spelling_checker_query_correction_map_%s.log'%(config.LOG_DIR, now)\n",
    "        checker.save_query_correction_map(fname)\n",
    "        # save data\n",
    "        logger.info('Save to %s'%config.ALL_DATA_LEMMATIZED)\n",
    "        columns_to_save = [col for col in dfAll.columns if col != 'product_attribute_concat']\n",
    "        pkl_utils._save(config.ALL_DATA_LEMMATIZED, dfAll[columns_to_save])\n",
    "\n",
    "    ## clean using stemmers\n",
    "    df_processor = pp.DataFrameParallelProcessor(pp.stemmers, config.DATA_PROCESSOR_N_JOBS)\n",
    "    df_processor.process(dfAll, columns_to_proc)\n",
    "    # split product_attribute_concat into product_attribute and product_attribute_list\n",
    "    dfAll['product_attribute'] = dfAll['product_attribute_concat'].apply(_split_attr_to_text)\n",
    "    dfAll['product_attribute_list'] = dfAll['product_attribute_concat'].apply(_split_attr_to_list)\n",
    "\n",
    "\n",
    "    # query expansion\n",
    "    if config.QUERY_EXPANSION:\n",
    "        list_processor = ListProcessor(stemmers)\n",
    "        base_stopwords = set(list_processor.process(list(config.STOP_WORDS)))\n",
    "        qe = QueryExpansion(dfAll, ngram=3, stopwords_threshold=0.9, base_stopwords=base_stopwords)\n",
    "        dfAll['search_term_alt'] = qe.build()\n",
    "        if config.TASK == 'sample':\n",
    "            print(dfAll[['search_term', 'search_term_alt']])\n",
    "\n",
    "    # save data\n",
    "    logger.info('Save to %s'%config.ALL_DATA_LEMMATIZED_STEMMED)\n",
    "    columns_to_save = [col for col in dfAll.columns if col != 'product_attribute_concat']\n",
    "    pkl_utils._save(config.ALL_DATA_LEMMATIZED_STEMMED, dfAll[columns_to_save])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading data_processor.ipynb in format ipynb\n",
      "[jupytext] Writing data_processor.py (destination file replaced)\n"
     ]
    }
   ],
   "source": [
    "# convert notebook.ipynb to a .py file\n",
    "!jupytext --to py data_processor.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
