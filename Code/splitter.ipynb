{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@author: Eric Tsai <eric492718@gmail.com>\\n@brief: splitter for Homedepot project\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Eric Tsai <eric492718@gmail.com>\n",
    "@brief: splitter for Homedepot project\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import sklearn.cross_validation\n",
    "# from sklearn.cross_validation import ShuffleSplit, _validate_shuffle_split\n",
    "# import sklearn.model_selection\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "plt.rcParams[\"figure.figsize\"] = [5, 5]\n",
    "\n",
    "import config\n",
    "from utils import pkl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "dfTrain = pd.read_csv(config.TRAIN_DATA, encoding=\"ISO-8859-1\")\n",
    "dfTest = pd.read_csv(config.TEST_DATA, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化參數\n",
    "n_splits=5\n",
    "random_state=config.RANDOM_SEED\n",
    "verbose=True\n",
    "plot=True\n",
    "# vaildatio 和 test的比例, XXX, XXX\n",
    "split_param=[0.5, 0.25, 0.5]\n",
    "col='search_term'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __str__():\n",
    "    return \"HomedepotSplitter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_split(dfTrain, dfTest, col, suffix=\"\", plot=False):\n",
    "    \"\"\" \n",
    "    1. calculate actual training and test data proportion, data is provided by Kaggle competition\n",
    "    2. calculate item proportion by a specific column. it can display three-item sets like training, test, and intersection item set\n",
    "    3. plot venn diagram to show the result\n",
    "    4. return specific column's unique items in train data (I think this step will let the function object be ambiguous, but I let it go.)\n",
    "    \"\"\"\n",
    "    #====================================================================================\n",
    "    if verbose:\n",
    "        print(\"-\"*50)\n",
    "    num_train = dfTrain.shape[0]\n",
    "    num_test = dfTest.shape[0]\n",
    "    ratio_train = num_train/(num_train+num_test)\n",
    "    ratio_test = num_test/(num_train+num_test)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Sample Stats: %.2f (train) | %.2f (test)\" % (ratio_train, ratio_test))\n",
    "    #====================================================================================\n",
    "    col_train_set = set(np.unique(dfTrain[col]))\n",
    "    col_test_set = set(np.unique(dfTest[col]))\n",
    "    col_total_set = col_train_set.union(col_test_set)\n",
    "    col_intersect_set = col_train_set.intersection(col_test_set)  # return col_train_set and col_test_set intersection\n",
    "\n",
    "    ratio_train = ((len(col_train_set) - len(col_intersect_set)) / len(col_total_set))  # only in train data\n",
    "    ratio_intersect = len(col_intersect_set) / len(col_total_set)  # set of intersection\n",
    "    ratio_test = ((len(col_test_set) - len(col_intersect_set)) / len(col_total_set))  # only in test data\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"%s Stats: %.2f (train) | %.2f (train & test) | %.2f (test)\" % (col, ratio_train, ratio_intersect, ratio_test))\n",
    "    #====================================================================================\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        if suffix == \"actual\":\n",
    "            venn2([col_train_set, col_test_set], (\"train\", \"test\"))\n",
    "        else:\n",
    "            venn2([col_train_set, col_test_set], (\"train\", \"valid\"))\n",
    "        fig_file = \"%s/%s_%s.pdf\"%(config.FIG_DIR, suffix, col)\n",
    "        plt.savefig(fig_file)\n",
    "        plt.clf()  # Clear figure\n",
    "    #====================================================================================\n",
    "    ## SORT it for reproducibility !!!\n",
    "    col_train_set = sorted(list(col_train_set))  # sorted(): default is sort by A to Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_value_set(df, col):\n",
    "    col_value_set = set(np.unique(df[col]))\n",
    "    return col_value_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_df_idx(df, col, values):\n",
    "    '''\n",
    "    if you get a list and contain values you want to find, \n",
    "    use this function to find the index from the data frame\n",
    "    '''\n",
    "    return np.where(df[col].isin(values))[0]  # note: this method return index in order, not actual index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split():\n",
    "    \"\"\"\n",
    "    object: to let your validation data and new training data same as old training data and test data in relationship patterns (search_term and product_id intersect set )\n",
    "    \"\"\"\n",
    "    ## original Train and Test Split\n",
    "    if verbose:\n",
    "        print(\"\\n\"+\"*\"*50)\n",
    "        print(\"Original Train and Test Split\")\n",
    "    _check_split(dfTrain, dfTest, col, \"actual\")\n",
    "    col_value_set = get_column_value_set(dfTrain, col)\n",
    "    col_value_set_li = list(col_value_set)\n",
    "    \n",
    "    ## naive split\n",
    "    if verbose:\n",
    "        print(\"\\n\"+\"*\"*50)\n",
    "        print(\"Naive Split\")\n",
    "\n",
    "    ## show naive split result(split like training and test data proportion which are Kaggle offer)    \n",
    "    rs = ShuffleSplit(n_splits=1, test_size=0.69, random_state=random_state)\n",
    "    for trainInd, validInd in rs.split(dfTrain):\n",
    "        dfTrain2 = dfTrain.iloc[trainInd].copy()\n",
    "        dfValid = dfTrain.iloc[validInd].copy()\n",
    "        _check_split(dfTrain2, dfValid, col, \"naive\")\n",
    "\n",
    "    \n",
    "    \n",
    "    ## split on product_uid & search_term and check which is better\n",
    "    if verbose:\n",
    "        print(\"\\n\"+\"*\"*50)\n",
    "        print(\"Split on product_uid & search_term\")\n",
    "    \n",
    "    splits = [0]*n_splits\n",
    "    rs = ShuffleSplit(n_splits=n_splits, test_size=split_param[0], random_state=random_state)\n",
    "    for run, (trInd, vaInd) in enumerate(rs.split(col_value_set_li)):\n",
    "        if verbose:\n",
    "            print(\"=\"*50)\n",
    "        # let some search term be a common term which is appear in training data and test data\n",
    "        ntr = int(len(trInd)*split_param[1])\n",
    "        term_train2 = [col_value_set_li[i] for i in trInd[:ntr]]\n",
    "        term_common = [col_value_set_li[i] for i in trInd[ntr:]]\n",
    "        term_valid = [col_value_set_li[i] for i in vaInd]\n",
    "\n",
    "        trainInd = _get_df_idx(dfTrain, col, term_train2)\n",
    "        commonInd = _get_df_idx(dfTrain, col, term_common)\n",
    "        validInd = _get_df_idx(dfTrain, col, term_valid)\n",
    "\n",
    "\n",
    "        # if search term only one value in data, then we need to split that feature without stratification\n",
    "        # if not, split it with stratification\n",
    "        dfTrain_index_reset = dfTrain.reset_index(drop=True)\n",
    "        dfTrain_feature_order_map = list(zip(dfTrain_index_reset.index.values, dfTrain_index_reset['search_term'].values))\n",
    "        group_feature = dfTrain_index_reset.iloc[commonInd].groupby(col).size().reset_index(name='size')\n",
    "        value = group_feature.query(\"size==1\")[col].values\n",
    "        commonInd_value_more_than_one = dfTrain_index_reset.iloc[commonInd][~dfTrain_index_reset.iloc[commonInd][col].isin(value)].index.values\n",
    "        commonInd_value_only_one = dfTrain_index_reset.iloc[commonInd][dfTrain_index_reset.iloc[commonInd][col].isin(value)].index.values\n",
    "\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=split_param[2], random_state=run)\n",
    "        iidx, oidx = list(sss.split(np.zeros(len(commonInd_value_more_than_one)), \n",
    "                                    dfTrain_index_reset.iloc[commonInd_value_more_than_one]['search_term'])\n",
    "                         )[0]\n",
    "        \n",
    "        trainInd = np.hstack((trainInd, commonInd_value_more_than_one[iidx]))  #np.hstack: Stack the arrays horizontally\n",
    "        validInd = np.hstack((validInd, commonInd_value_more_than_one[oidx]))\n",
    "\n",
    "        ss = ShuffleSplit(n_splits=1, test_size=split_param[2], random_state=run)\n",
    "        iidx, oidx = list(ss.split(np.zeros(len(commonInd_value_only_one))))[0]\n",
    "        trainInd = np.hstack((trainInd, commonInd_value_only_one[iidx]))\n",
    "        validInd = np.hstack((validInd, commonInd_value_only_one[oidx]))\n",
    "\n",
    "\n",
    "#         sss = StratifiedShuffleSplit(dfTrain.iloc[commonInd][\"search_term\"], \n",
    "#         n_splits=1, test_size=split_param[2], random_state=run)\n",
    "#         iidx, oidx = list(sss.split(np.zeros(len(dfTrain.iloc[commonInd][col])), dfTrain.iloc[commonInd][col]))[0]\n",
    "\n",
    "#         trainInd = np.hstack((trainInd, commonInd[iidx]))\n",
    "#         validInd = np.hstack((validInd, commonInd[oidx]))\n",
    "\n",
    "        trainInd = sorted(trainInd)\n",
    "        validInd = sorted(validInd)\n",
    "\n",
    "        if verbose:\n",
    "            dfTrain2 = dfTrain_index_reset.iloc[trainInd].copy()\n",
    "            dfValid = dfTrain_index_reset.iloc[validInd].copy()\n",
    "            if run == 0:\n",
    "                plot = True\n",
    "            else:\n",
    "                plot = False\n",
    "            _check_split(dfTrain2, dfValid, col, \"proposed\", plot)\n",
    "#             _check_split(dfTrain2, dfValid, \"search_term\", \"proposed\", plot)\n",
    "\n",
    "        splits[run] = (trainInd, validInd)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-\"*50)\n",
    "            print(\"Index for run: %s\" % (run+1))\n",
    "            print(\"Train (num = %s)\" % len(trainInd))\n",
    "            print(trainInd[:10])\n",
    "            print(\"Valid (num = %s)\" % len(validInd))\n",
    "            print(validInd[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.09 (train) | 0.39 (train & test) | 0.52 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.05 (train) | 0.75 (train & test) | 0.20 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.15 (train) | 0.33 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 23202)\n",
      "[6, 11, 12, 15, 19, 27, 34, 35, 36, 37]\n",
      "Valid (num = 50865)\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10]\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.15 (train) | 0.33 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 2\n",
      "Train (num = 23008)\n",
      "[11, 12, 17, 18, 19, 22, 27, 28, 30, 32]\n",
      "Valid (num = 51059)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.15 (train) | 0.33 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 3\n",
      "Train (num = 23209)\n",
      "[1, 6, 9, 10, 13, 21, 23, 24, 27, 29]\n",
      "Valid (num = 50858)\n",
      "[0, 2, 3, 4, 5, 7, 8, 11, 12, 14]\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.15 (train) | 0.33 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 4\n",
      "Train (num = 23123)\n",
      "[1, 4, 10, 22, 25, 29, 35, 40, 42, 43]\n",
      "Valid (num = 50944)\n",
      "[0, 2, 3, 5, 6, 7, 8, 9, 11, 12]\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.15 (train) | 0.33 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 5\n",
      "Train (num = 23228)\n",
      "[5, 7, 8, 9, 14, 17, 25, 27, 31, 32]\n",
      "Valid (num = 50839)\n",
      "[0, 1, 2, 3, 4, 6, 10, 11, 12, 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code style=\"background:yellow;color:black\">***In the above result, we find out using the search_term column to split the data that help us to simulate test data pattern on validation data.***</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitter Modular Design\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## advanced splitter\n",
    "class HomedepotSplitter:\n",
    "    def __init__(self, dfTrain, dfTest, col, n_splits=5, random_state=config.RANDOM_SEED,\n",
    "                    verbose=False, plot=False, split_param=[0.5, 0.25, 0.5]):\n",
    "        self.dfTrain = dfTrain\n",
    "        self.dfTest = dfTest\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.plot = plot\n",
    "        self.split_param = split_param\n",
    "        self.col = col\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"HomedepotSplitter\"\n",
    "\n",
    "    def _check_split(self, dfTrain, dfTest, col, suffix=\"\", plot=False):\n",
    "        \"\"\" \n",
    "        1. calculate actual training and test data proportion, data is provided by Kaggle competition\n",
    "        2. calculate item proportion by a specific column. it can display three-item sets like training, test, and intersection item set\n",
    "        3. plot venn diagram to show the result\n",
    "        4. return specific column's unique items in train data (I think this step will let the function object be ambiguous, but I let it go.)\n",
    "        \"\"\"\n",
    "        #====================================================================================\n",
    "        if self.verbose:\n",
    "            print(\"-\"*50)\n",
    "        num_train = self.dfTrain.shape[0]\n",
    "        num_test = self.dfTest.shape[0]\n",
    "        ratio_train = num_train/(num_train+num_test)\n",
    "        ratio_test = num_test/(num_train+num_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Sample Stats: %.2f (train) | %.2f (test)\" % (ratio_train, ratio_test))\n",
    "        #====================================================================================\n",
    "        col_train_set = set(np.unique(self.dfTrain[self.col]))\n",
    "        col_test_set = set(np.unique(self.dfTest[self.col]))\n",
    "        col_total_set = col_train_set.union(col_test_set)\n",
    "        col_intersect_set = col_train_set.intersection(col_test_set)  # return col_train_set and col_test_set intersection\n",
    "\n",
    "        ratio_train = ((len(col_train_set) - len(col_intersect_set)) / len(col_total_set))  # only in train data\n",
    "        ratio_intersect = len(col_intersect_set) / len(col_total_set)  # set of intersection\n",
    "        ratio_test = ((len(col_test_set) - len(col_intersect_set)) / len(col_total_set))  # only in test data\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"%s Stats: %.2f (train) | %.2f (train & test) | %.2f (test)\" % (self.col, ratio_train, ratio_intersect, ratio_test))\n",
    "        #====================================================================================\n",
    "        if self.plot:\n",
    "            plt.figure()\n",
    "            if suffix == \"actual\":\n",
    "                venn2([col_train_set, col_test_set], (\"train\", \"test\"))\n",
    "            else:\n",
    "                venn2([col_train_set, col_test_set], (\"train\", \"valid\"))\n",
    "            fig_file = \"%s/%s_%s.pdf\"%(config.FIG_DIR, suffix, self.col)\n",
    "            plt.savefig(fig_file)\n",
    "            plt.clf()  # Clear figure\n",
    "        #====================================================================================\n",
    "        ## SORT it for reproducibility !!!\n",
    "        col_train_set = sorted(list(col_train_set))  # sorted(): default is sort by A to Z\n",
    "        return col_train_set\n",
    "\n",
    "    def _get_df_idx(self, df, col, values):\n",
    "        '''\n",
    "        if you get a list and contain item we want to find, \n",
    "        use this function to find the index from the data frame\n",
    "        '''\n",
    "        return np.where(df[self.col].isin(values))[0]  # note: this method return index in order, not actual index\n",
    "    \n",
    "    def get_column_value_set(self, df, col):\n",
    "        col_value_set = set(np.unique(df[col]))\n",
    "        return col_value_set\n",
    "    \n",
    "    def split(self):\n",
    "        \"\"\"\n",
    "        object: to let your validation data and new training data same as old training data and test data in\n",
    "        relationship patterns (search_term and product_id intersect set )\n",
    "        \"\"\"\n",
    "        ## original Train and Test Split\n",
    "        if self.verbose:\n",
    "            print(\"\\n\"+\"*\"*50)\n",
    "            print(\"Original Train and Test Split\")\n",
    "        self._check_split(self.dfTrain, self.dfTest, self.col, \"actual\")\n",
    "        col_value_set = self.get_column_value_set(self.dfTrain, self.col)\n",
    "        col_value_set_li = list(col_value_set)\n",
    "\n",
    "        ## naive split\n",
    "        if self.verbose:\n",
    "            print(\"\\n\"+\"*\"*50)\n",
    "            print(\"Naive Split\")\n",
    "\n",
    "        ## show naive split result(split like training and test data proportion which are Kaggle offer)    \n",
    "        rs = ShuffleSplit(n_splits=1, test_size=0.69, random_state=self.random_state)\n",
    "        for trainInd, validInd in rs.split(self.dfTrain):\n",
    "            dfTrain2 = self.dfTrain.iloc[trainInd].copy()\n",
    "            dfValid = self.dfTrain.iloc[validInd].copy()\n",
    "            self._check_split(dfTrain2, dfValid, self.col, \"naive\")\n",
    "\n",
    "            \n",
    "        ## split on product_uid & search_term and check which is better\n",
    "        if self.verbose:\n",
    "            print(\"\\n\"+\"*\"*50)\n",
    "            print(\"Split on product_uid & search_term\")\n",
    "\n",
    "        self.splits = [0]*self.n_splits\n",
    "        rs = ShuffleSplit(n_splits=self.n_splits, test_size=self.split_param[0], random_state=self.random_state)\n",
    "        for run, (trInd, vaInd) in enumerate(rs.split(col_value_set_li)):\n",
    "            if self.verbose:\n",
    "                print(\"=\"*50)\n",
    "            # let some search term be a common term which is appear in training data and test data\n",
    "            ntr = int(len(trInd)*self.split_param[1])\n",
    "            term_train2 = [col_value_set_li[i] for i in trInd[:ntr]]\n",
    "            term_common = [col_value_set_li[i] for i in trInd[ntr:]]\n",
    "            term_valid = [col_value_set_li[i] for i in vaInd]\n",
    "\n",
    "            trainInd = self._get_df_idx(self.dfTrain, self.col, term_train2)\n",
    "            commonInd = self._get_df_idx(self.dfTrain, self.col, term_common)\n",
    "            validInd = self._get_df_idx(self.dfTrain, self.col, term_valid)\n",
    "\n",
    "\n",
    "            # if search term only one value in data, then we need to split that feature without stratification\n",
    "            # if not, split it with stratification\n",
    "            dfTrain_index_reset = self.dfTrain.reset_index(drop=True)\n",
    "            dfTrain_feature_order_map = list(zip(dfTrain_index_reset.index.values, dfTrain_index_reset['search_term'].values))\n",
    "            group_feature = dfTrain_index_reset.iloc[commonInd].groupby(self.col).size().reset_index(name='size')\n",
    "            value = group_feature.query(\"size==1\")[self.col].values\n",
    "            commonInd_value_more_than_one = dfTrain_index_reset.iloc[commonInd][~dfTrain_index_reset.iloc[commonInd][self.col].isin(value)].index.values\n",
    "            commonInd_value_only_one = dfTrain_index_reset.iloc[commonInd][dfTrain_index_reset.iloc[commonInd][self.col].isin(value)].index.values\n",
    "\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=self.split_param[2], random_state=run)\n",
    "            iidx, oidx = list(sss.split(np.zeros(len(commonInd_value_more_than_one)), \n",
    "                                        dfTrain_index_reset.iloc[commonInd_value_more_than_one]['search_term'])\n",
    "                             )[0]\n",
    "\n",
    "            trainInd = np.hstack((trainInd, commonInd_value_more_than_one[iidx]))  #np.hstack: Stack the arrays horizontally\n",
    "            validInd = np.hstack((validInd, commonInd_value_more_than_one[oidx]))\n",
    "\n",
    "            ss = ShuffleSplit(n_splits=1, test_size=self.split_param[2], random_state=run)\n",
    "            iidx, oidx = list(ss.split(np.zeros(len(commonInd_value_only_one))))[0]\n",
    "            trainInd = np.hstack((trainInd, commonInd_value_only_one[iidx]))\n",
    "            validInd = np.hstack((validInd, commonInd_value_only_one[oidx]))\n",
    "\n",
    "\n",
    "            trainInd = sorted(trainInd)\n",
    "            validInd = sorted(validInd)\n",
    "\n",
    "            if self.verbose:\n",
    "                dfTrain2 = dfTrain_index_reset.iloc[trainInd].copy()\n",
    "                dfValid = dfTrain_index_reset.iloc[validInd].copy()\n",
    "                if run == 0:\n",
    "                    plot = True\n",
    "                else:\n",
    "                    plot = False\n",
    "                self._check_split(dfTrain2, dfValid, self.col, \"proposed\", plot)\n",
    "    #             _check_split(dfTrain2, dfValid, \"search_term\", \"proposed\", plot)\n",
    "\n",
    "\n",
    "            self.splits[run] = trainInd, validInd\n",
    "        \n",
    "            if self.verbose:\n",
    "                print(\"-\"*50)\n",
    "                print(\"Index for run: %s\" % (run+1))\n",
    "                print(\"Train (num = %s)\" % len(trainInd))\n",
    "                print(trainInd[:10])\n",
    "                print(\"Valid (num = %s)\" % len(validInd))\n",
    "                print(validInd[:10])\n",
    "        return self\n",
    "    \n",
    "    def save(self, fname):\n",
    "        pkl_utils._save(fname, self.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dfTrain = pd.read_csv(config.TRAIN_DATA, encoding=\"ISO-8859-1\")\n",
    "    dfTest = pd.read_csv(config.TEST_DATA, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    # splits for level1\n",
    "    splitter = HomedepotSplitter(dfTrain=dfTrain, \n",
    "                                 dfTest=dfTest,\n",
    "                                 col = 'search_term',\n",
    "                                 n_splits=config.N_RUNS, \n",
    "                                 random_state=config.RANDOM_SEED, \n",
    "                                 verbose=True,\n",
    "                                 plot=True,\n",
    "                                 # tune these params to get a close distribution\n",
    "                                 split_param=[0.5, 0.25, 0.5],\n",
    "                                )\n",
    "    splitter.split()\n",
    "    splitter.save(\"%s/splits_level1.pkl\"%config.SPLIT_DIR)\n",
    "    splits_level1 = splitter.splits\n",
    "    \n",
    "    ## splits for level2\n",
    "    splits_level1 = pkl_utils._load(\"%s/splits_level1.pkl\"%config.SPLIT_DIR)\n",
    "    splits_level2 = [0]*config.N_RUNS\n",
    "    for run, (trainInd, validInd) in enumerate(splits_level1):\n",
    "        dfValid = dfTrain.iloc[validInd].copy()\n",
    "        splitter2 = HomedepotSplitter(dfTrain=dfValid, \n",
    "                                      dfTest=dfTest, \n",
    "                                      col = 'search_term',\n",
    "                                      n_splits=1, \n",
    "                                      random_state=run, \n",
    "                                      verbose=True,\n",
    "                                      # tune these params to get a close distribution\n",
    "                                      split_param=[0.5, 0.15, 0.6]\n",
    "                                     )\n",
    "        splitter2.split()\n",
    "        splits_level2[run] = splitter2.splits[0]\n",
    "    pkl_utils._save(\"%s/splits_level2.pkl\"%config.SPLIT_DIR, splits_level2)\n",
    "\n",
    "    ## splits for level3\n",
    "    splits_level2 = pkl_utils._load(\"%s/splits_level2.pkl\"%config.SPLIT_DIR)\n",
    "    splits_level3 = [0]*config.N_RUNS\n",
    "    for run, (trainInd, validInd) in enumerate(splits_level2):\n",
    "        dfValid = dfTrain.iloc[validInd].copy()\n",
    "        splitter3 = HomedepotSplitter(dfTrain=dfValid, \n",
    "                                    dfTest=dfTest,\n",
    "                                    col = 'search_term',\n",
    "                                    n_splits=1, \n",
    "                                    random_state=run, \n",
    "                                    verbose=True,\n",
    "                                    # tune these params to get a close distribution\n",
    "                                    split_param=[0.5, 0.15, 0.7])\n",
    "        splitter3.split()\n",
    "        splits_level3[run] = splitter3.splits[0]\n",
    "    pkl_utils._save(\"%s/splits_level3.pkl\"%config.SPLIT_DIR, splits_level3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.09 (train) | 0.39 (train & test) | 0.52 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.09 (train) | 0.39 (train & test) | 0.52 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.09 (train) | 0.39 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 23202)\n",
      "[6, 11, 12, 15, 19, 27, 34, 35, 36, 37]\n",
      "Valid (num = 50865)\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9, 10]\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.09 (train) | 0.39 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 2\n",
      "Train (num = 23008)\n",
      "[11, 12, 17, 18, 19, 22, 27, 28, 30, 32]\n",
      "Valid (num = 51059)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.09 (train) | 0.39 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 3\n",
      "Train (num = 23209)\n",
      "[1, 6, 9, 10, 13, 21, 23, 24, 27, 29]\n",
      "Valid (num = 50858)\n",
      "[0, 2, 3, 4, 5, 7, 8, 11, 12, 14]\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.09 (train) | 0.39 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 4\n",
      "Train (num = 23123)\n",
      "[1, 4, 10, 22, 25, 29, 35, 40, 42, 43]\n",
      "Valid (num = 50944)\n",
      "[0, 2, 3, 5, 6, 7, 8, 9, 11, 12]\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.31 (train) | 0.69 (test)\n",
      "search_term Stats: 0.09 (train) | 0.39 (train & test) | 0.52 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 5\n",
      "Train (num = 23228)\n",
      "[5, 7, 8, 9, 14, 17, 25, 27, 31, 32]\n",
      "Valid (num = 50839)\n",
      "[0, 1, 2, 3, 4, 6, 10, 11, 12, 13]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 12496)\n",
      "[2, 10, 22, 23, 32, 36, 43, 45, 54, 55]\n",
      "Valid (num = 38369)\n",
      "[0, 1, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 12440)\n",
      "[0, 14, 16, 19, 20, 21, 25, 29, 32, 39]\n",
      "Valid (num = 38619)\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 12457)\n",
      "[4, 11, 18, 19, 25, 26, 29, 34, 43, 44]\n",
      "Valid (num = 38401)\n",
      "[0, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 12583)\n",
      "[1, 4, 8, 10, 15, 17, 24, 26, 27, 29]\n",
      "Valid (num = 38361)\n",
      "[0, 2, 3, 5, 6, 7, 9, 11, 12, 13]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.23 (train) | 0.77 (test)\n",
      "search_term Stats: 0.07 (train) | 0.34 (train & test) | 0.58 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 12330)\n",
      "[5, 13, 20, 28, 30, 34, 35, 36, 40, 41]\n",
      "Valid (num = 38509)\n",
      "[0, 1, 2, 3, 4, 6, 7, 8, 9, 10]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 7776)\n",
      "[21, 23, 26, 43, 48, 49, 51, 55, 59, 64]\n",
      "Valid (num = 30593)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.35 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.35 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.35 (train & test) | 0.59 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 7823)\n",
      "[2, 6, 9, 11, 19, 25, 26, 31, 36, 37]\n",
      "Valid (num = 30796)\n",
      "[0, 1, 3, 4, 5, 7, 8, 10, 12, 13]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 7654)\n",
      "[5, 9, 23, 26, 32, 33, 68, 74, 83, 86]\n",
      "Valid (num = 30747)\n",
      "[0, 1, 2, 3, 4, 6, 7, 8, 10, 11]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 7885)\n",
      "[2, 6, 10, 14, 16, 17, 23, 33, 36, 37]\n",
      "Valid (num = 30476)\n",
      "[0, 1, 3, 4, 5, 7, 8, 9, 11, 12]\n",
      "\n",
      "**************************************************\n",
      "Original Train and Test Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Naive Split\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "\n",
      "**************************************************\n",
      "Split on product_uid & search_term\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "Sample Stats: 0.19 (train) | 0.81 (test)\n",
      "search_term Stats: 0.06 (train) | 0.36 (train & test) | 0.59 (test)\n",
      "--------------------------------------------------\n",
      "Index for run: 1\n",
      "Train (num = 7920)\n",
      "[1, 2, 6, 10, 14, 21, 28, 30, 48, 54]\n",
      "Valid (num = 30589)\n",
      "[0, 3, 4, 5, 7, 8, 9, 11, 12, 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading splitter.ipynb in format ipynb\n",
      "[jupytext] Writing splitter.py\n"
     ]
    }
   ],
   "source": [
    "# convert notebook.ipynb to a .py file\n",
    "!jupytext --to py splitter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
