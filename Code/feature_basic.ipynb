{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@author: Eric Tsai <eric492718@gmail.com>\\n@brief: basic features\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Eric Tsai <eric492718@gmail.com>\n",
    "@brief: basic features\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import config\n",
    "from utils import ngram_utils, nlp_utils, np_utils\n",
    "from utils import time_utils, logging_utils, pkl_utils\n",
    "from feature_base import BaseEstimator, StandaloneFeatureWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the token pattern to get a better correlation with y_train\n",
    "# token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "# token_pattern = r\"\\w{1,}\"\n",
    "# token_pattern = r\"\\w+\"\n",
    "# token_pattern = r\"[\\w']+\"\n",
    "token_pattern = \" \" # just split the text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocId(BaseEstimator):\n",
    "    \"\"\"\n",
    "    1. find the item set\n",
    "    2. Assign values in order starting from 0\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "        obs_set = set(obs_corpus)\n",
    "        self.encoder = dict(zip(obs_set, range(len(obs_set))))\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"DocId\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        return self.encoder[obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocIdEcho(BaseEstimator):\n",
    "    \"\"\"\n",
    "    return the original Id of obs; this is only for product uid\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"DocIdEcho\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocIdOneHot(BaseEstimator):\n",
    "    \"\"\"\n",
    "    1. one hot encoding \n",
    "    2. for linear model\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"DocIdOneHot\"\n",
    "\n",
    "    def transform(self):\n",
    "        lb = LabelBinarizer(sparse_output=True)  # sparse_output=True: let output be in sparse CSR format to reduce storage space\n",
    "        return lb.fit_transform(self.obs_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nproduct_uid     int(obs > 164038 and obs <= 206650)\\nid              int(obs > 163700 and obs <= 221473)\\nIn test, we have\\n#sample = 147406 for product_uid <= 206650\\n#sample = 19287 for product_uid\\nThe majority will be in the 1st and 2nd parts.\\nIn specific,\\n50K points of 147406 in public, and the rest 100K points in private.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "product_uid     int(obs > 164038 and obs <= 206650)\n",
    "id              int(obs > 163700 and obs <= 221473)\n",
    "In test, we have\n",
    "#sample = 147406 for product_uid <= 206650\n",
    "#sample = 19287 for product_uid\n",
    "The majority will be in the 1st and 2nd parts.\n",
    "In specific,\n",
    "50K points of 147406 in public, and the rest 100K points in private.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductUidDummy1(BaseEstimator):\n",
    "    \"\"\"For product_uid\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"ProductUidDummy1\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        return int(obs<163800)  # convert boolean value to interger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductUidDummy2(BaseEstimator):\n",
    "    \"\"\"For product_uid\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"ProductUidDummy2\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        return int(obs>206650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductUidDummy3(BaseEstimator):\n",
    "    \"\"\"For product_uid\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"ProductUidDummy3\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        return int(obs > 164038 and obs <= 206650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocLen(BaseEstimator):\n",
    "    \"\"\"Length of document\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"DocLen\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        obs_tokens = nlp_utils._tokenize(obs, token_pattern)\n",
    "        return len(obs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocFreq(BaseEstimator):\n",
    "    \"\"\"Frequency of the document in the corpus\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "        self.counter = Counter(obs_corpus)  # Count the number of occurrences of value \n",
    "\n",
    "    def __name__(self):\n",
    "        return \"DocFreq\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        return self.counter[obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocEntropy(BaseEstimator):\n",
    "    \"\"\"Entropy of the document\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"DocEntropy\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        obs_tokens = nlp_utils._tokenize(obs, token_pattern)\n",
    "        counter = Counter(obs_tokens)\n",
    "        count = np.asarray(list(counter.values()))\n",
    "        proba = count/np.sum(count)\n",
    "        return np_utils._entropy(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCount(BaseEstimator):\n",
    "    \"\"\"Count of digit in the document\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"DigitCount\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        return len(re.findall(r\"\\d\", obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRatio(BaseEstimator):\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"DigitRatio\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        obs_tokens = nlp_utils._tokenize(obs, token_pattern)\n",
    "        return np_utils._try_divide(len(re.findall(r\"\\d\", obs)), len(obs_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueCount_Ngram(BaseEstimator):\n",
    "    def __init__(self, obs_corpus, target_corpus, ngram, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "        self.ngram = ngram\n",
    "        self.ngram_str = ngram_utils._ngram_str_map[self.ngram]\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"UniqueCount_%s\"%self.ngram_str\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        obs_tokens = nlp_utils._tokenize(obs, token_pattern)\n",
    "        obs_ngrams = ngram_utils._ngrams(obs_tokens, self.ngram)\n",
    "        return len(set(obs_ngrams))\n",
    "\n",
    "\n",
    "class UniqueRatio_Ngram(BaseEstimator):\n",
    "    def __init__(self, obs_corpus, target_corpus, ngram, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "        self.ngram = ngram\n",
    "        self.ngram_str = ngram_utils._ngram_str_map[self.ngram]\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"UniqueRatio_%s\"%self.ngram_str\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        obs_tokens = nlp_utils._tokenize(obs, token_pattern)\n",
    "        obs_ngrams = ngram_utils._ngrams(obs_tokens, self.ngram)\n",
    "        return np_utils._try_divide(len(set(obs_ngrams)), len(obs_ngrams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------- Attribute based features ----------------------\n",
    "class AttrCount(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrCount\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        return len(obs)\n",
    "class AttrBulletCount(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrBulletCount\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        cnt = 0\n",
    "        for lst in obs:\n",
    "            if lst[0].startswith(\"bullet\"):\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "\n",
    "\n",
    "class AttrBulletRatio(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrBulletRatio\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        cnt = 0\n",
    "        for lst in obs:\n",
    "            if lst[0].startswith(\"bullet\"):\n",
    "                cnt += 1\n",
    "        return np_utils._try_divide(cnt, len(obs))\n",
    "\n",
    "\n",
    "class AttrNonBulletCount(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrNonBulletCount\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        cnt = 0\n",
    "        for lst in obs:\n",
    "            if not lst[0].startswith(\"bullet\"):\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "\n",
    "\n",
    "class AttrNonBulletRatio(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrNonBulletRatio\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        cnt = 0\n",
    "        for lst in obs:\n",
    "            if not lst[0].startswith(\"bullet\"):\n",
    "                cnt += 1\n",
    "        return np_utils._try_divide(cnt, len(obs))\n",
    "\n",
    "\n",
    "class AttrHasProductHeight(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrHasProductHeight\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        for lst in obs:\n",
    "            if lst[0].find(\"product height\") != -1:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "class AttrHasProductWidth(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrHasProductWidth\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        for lst in obs:\n",
    "            if lst[0].find(\"product width\") != -1:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "class AttrHasProductLength(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrHasProductLength\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        for lst in obs:\n",
    "            if lst[0].find(\"product length\") != -1:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "class AttrHasProductDepth(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrHasProductDepth\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        for lst in obs:\n",
    "            if lst[0].find(\"product depth\") != -1:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "\n",
    "class AttrHasIndoorOutdoor(BaseEstimator):\n",
    "    \"\"\"obs_corpus is a list of list of attributes\"\"\"\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode=\"\"):\n",
    "        super().__init__(obs_corpus, target_corpus, aggregation_mode)\n",
    "\n",
    "    def __name__(self):\n",
    "        return \"AttrHasIndoorOutdoor\"\n",
    "\n",
    "    def transform_one(self, obs, target, id):\n",
    "        \"\"\"obs is a list of attributes\"\"\"\n",
    "        for lst in obs:\n",
    "            if lst[0].find(\"indoor outdoor\") != -1:\n",
    "                return 1\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\HomeDepotProductSearchRelevance\\lib\\site-packages\\scipy\\stats\\stats.py:3913: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "#---------------- Main ---------------------------\n",
    "def main():\n",
    "    logname = \"generate_feature_basic_%s.log\"%time_utils._timestamp()\n",
    "    logger = logging_utils._get_logger(config.LOG_DIR, logname)\n",
    "    dfAll = pkl_utils._load(config.ALL_DATA_LEMMATIZED_STEMMED)\n",
    "\n",
    "    ## basic\n",
    "    generators = [DocId, DocLen, DocFreq, DocEntropy, DigitCount, DigitRatio]\n",
    "    obs_fields = [\"search_term\", \"product_title\", \"product_description\", \n",
    "                \"product_attribute\", \"product_brand\", \"product_color\"]\n",
    "    for generator in generators:\n",
    "        param_list = []\n",
    "        sf = StandaloneFeatureWrapper(generator, dfAll, obs_fields, param_list, config.FEAT_DIR, logger)\n",
    "        sf.go()\n",
    "\n",
    "    ## for product_uid\n",
    "    generators = [DocIdEcho, DocFreq, ProductUidDummy1, ProductUidDummy2, ProductUidDummy3]\n",
    "    obs_fields = [\"product_uid\"]\n",
    "    for generator in generators:\n",
    "        param_list = []\n",
    "        sf = StandaloneFeatureWrapper(generator, dfAll, obs_fields, param_list, config.FEAT_DIR, logger)\n",
    "        sf.go()\n",
    "\n",
    "    ## unique count\n",
    "    generators = [UniqueCount_Ngram, UniqueRatio_Ngram]\n",
    "    obs_fields = [\"search_term\", \"product_title\", \"product_description\", \n",
    "    \"product_attribute\", \"product_brand\", \"product_color\"]\n",
    "    ngrams = [1,2,3]\n",
    "    for generator in generators:\n",
    "        for ngram in ngrams:\n",
    "            param_list = [ngram]\n",
    "            sf = StandaloneFeatureWrapper(generator, dfAll, obs_fields, param_list, config.FEAT_DIR, logger)\n",
    "            sf.go()\n",
    "\n",
    "    ## for product_attribute_list\n",
    "    generators = [\n",
    "        AttrCount, \n",
    "        AttrBulletCount, \n",
    "        AttrBulletRatio, \n",
    "        AttrNonBulletCount, \n",
    "        AttrNonBulletRatio,\n",
    "        AttrHasProductHeight,\n",
    "        AttrHasProductWidth,\n",
    "        AttrHasProductLength,\n",
    "        AttrHasProductDepth,\n",
    "        AttrHasIndoorOutdoor,\n",
    "    ]\n",
    "    obs_fields = [\"product_attribute_list\"]\n",
    "    for generator in generators:\n",
    "        param_list = []\n",
    "        sf = StandaloneFeatureWrapper(generator, dfAll, obs_fields, param_list, config.FEAT_DIR, logger)\n",
    "        sf.go()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading feature_basic.ipynb in format ipynb\n",
      "[jupytext] Writing feature_basic.py (destination file replaced)\n"
     ]
    }
   ],
   "source": [
    "# convert notebook.ipynb to a .py file\n",
    "!jupytext --to py feature_basic.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
