{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effective-stake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@author: Eric Tsain <eric492718g@gmail.com>\\n@brief: base class for feature generation\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Eric Tsain <eric492718g@gmail.com>\n",
    "@brief: base class for feature generation\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deadly-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import config\n",
    "from config import TRAIN_SIZE\n",
    "from utils import np_utils, pkl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disturbed-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have many features that measure the correlation/similarity/distance\n",
    "# between search_term and product_title/product_description, we implement this base class.\n",
    "class BaseEstimator:\n",
    "    def __init__(self, obs_corpus, target_corpus, aggregation_mode, id_list=None, aggregation_mode_prev=\"\"):\n",
    "        self.obs_corpus = obs_corpus\n",
    "        self.N = len(obs_corpus)\n",
    "        # for standalone feature, we use the same interface, so better take care of it\n",
    "        self.target_corpus = range(self.N) if target_corpus is None else target_corpus\n",
    "        # id_list is used for group based relevance/distance features\n",
    "        self.id_list = range(self.N) if id_list is None else id_list\n",
    "        # aggregation for list features, e.g., intersect positions\n",
    "        self.aggregation_mode, self.aggregator = self._check_aggregation_mode(aggregation_mode)\n",
    "        self.aggregation_mode_prev, self.aggregator_prev = self._check_aggregation_mode(aggregation_mode_prev)\n",
    "        self.double_aggregation = False\n",
    "        if self.aggregator_prev != [None]:\n",
    "            # the output of transform_one is a list of list, i.e., [[...], [...], [...]]\n",
    "            # self.aggregator_prev is used to aggregate the inner list\n",
    "            # This is used for the following features:\n",
    "            # 1. EditDistance_Ngram\n",
    "            # 2. CompressionDistance_Ngram\n",
    "            # 3. Word2Vec_CosineSim\n",
    "            # 4. WordNet_Path_Similarity, WordNet_Lch_Similarity, WordNet_Wup_Similarity\n",
    "            # which are very time consuming to compute the inner list\n",
    "            self.double_aggregation = True\n",
    "\n",
    "    def _check_aggregation_mode(self, aggregation_mode):\n",
    "        \"\"\"\n",
    "        Confirm whether the aggregation mode is filled in correctly\n",
    "        \"\"\"\n",
    "        valid_aggregation_modes = [\"\", \"size\", \"mean\", \"std\", \"max\", \"min\", \"median\"]\n",
    "        if isinstance(aggregation_mode, str):\n",
    "            assert aggregation_mode.lower() in valid_aggregation_modes, \"Wrong aggregation_mode: %s\"%aggregation_mode\n",
    "            aggregation_mode = [aggregation_mode.lower()]\n",
    "        elif isinstance(aggregation_mode, list):\n",
    "            for m in aggregation_mode:\n",
    "                assert m.lower() in valid_aggregation_modes, \"Wrong aggregation_mode: %s\"%m\n",
    "            aggregation_mode = [m.lower() for m in aggregation_mode]\n",
    "\n",
    "        aggregator = [None if m == \"\" else getattr(np, m) for m in aggregation_mode]\n",
    "\n",
    "        return aggregation_mode, aggregator\n",
    "\n",
    "    def transform(self):\n",
    "        # original score\n",
    "        score = list(map(self.transform_one, self.obs_corpus, self.target_corpus, self.id_list))\n",
    "        # aggregation\n",
    "        if isinstance(score[0], list):\n",
    "            if self.double_aggregation:\n",
    "                # double aggregation\n",
    "                res = np.zeros((self.N, len(self.aggregator_prev) * len(self.aggregator)), dtype=float)\n",
    "                for m,aggregator_prev in enumerate(self.aggregator_prev):\n",
    "                    for n,aggregator in enumerate(self.aggregator):\n",
    "                        idx = m * len(self.aggregator) + n\n",
    "                        for i in range(self.N):\n",
    "                            # process in a safer way\n",
    "                            try:\n",
    "                                tmp = []\n",
    "                                for l in score[i]:\n",
    "                                    try:\n",
    "                                        s = aggregator_prev(l)\n",
    "                                    except:\n",
    "                                        s = config.MISSING_VALUE_NUMERIC\n",
    "                                    tmp.append(s)\n",
    "                            except:\n",
    "                                tmp = [ config.MISSING_VALUE_NUMERIC ]\n",
    "                            try:\n",
    "                                s = aggregator(tmp)\n",
    "                            except:\n",
    "                                s = config.MISSING_VALUE_NUMERIC\n",
    "                            res[i,idx] = s\n",
    "            else:\n",
    "                # single aggregation\n",
    "                res = np.zeros((self.N, len(self.aggregator)), dtype=float)\n",
    "                for m,aggregator in enumerate(self.aggregator):\n",
    "                    for i in range(self.N):\n",
    "                        # process in a safer way\n",
    "                        try:\n",
    "                            s = aggregator(score[i])\n",
    "                        except:\n",
    "                            s = config.MISSING_VALUE_NUMERIC\n",
    "                        res[i,m] = s\n",
    "        else:\n",
    "            res = np.asarray(score, dtype=float)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "consolidated-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for generating standalone feature, e.g., \n",
    "# count of words in search_term\n",
    "class StandaloneFeatureWrapper:\n",
    "    \"\"\"\n",
    "    1. Load label from training data.\n",
    "    2. Check obs_fields(column). If it doesn't exist, skip the loop and write a record into a log file\n",
    "    3. transform corpus to feature\n",
    "    4. if feature dimension is one than caculate correlation\n",
    "    \"\"\"\n",
    "    def __init__(self, generator, dfAll, obs_fields, param_list, feat_dir, logger, force_corr=False):\n",
    "        self.generator = generator\n",
    "        self.dfAll = dfAll\n",
    "        self.obs_fields = obs_fields\n",
    "        self.param_list = param_list\n",
    "        self.feat_dir = feat_dir\n",
    "        self.logger = logger\n",
    "        self.force_corr = force_corr\n",
    "\n",
    "    def go(self):\n",
    "        y_train = self.dfAll[\"relevance\"].values[:TRAIN_SIZE]\n",
    "        for obs_field in self.obs_fields:\n",
    "            if obs_field not in self.dfAll.columns:\n",
    "                self.logger.info(\"Skip %s\"%obs_field)\n",
    "                continue\n",
    "            obs_corpus = self.dfAll[obs_field].values\n",
    "            ext = self.generator(obs_corpus, None, *self.param_list)\n",
    "            x = ext.transform()\n",
    "            if isinstance(ext.__name__(), list):\n",
    "                for i,feat_name in enumerate(ext.__name__()):\n",
    "                    dim = 1\n",
    "                    fname = \"%s_%s_%dD\"%(feat_name, obs_field, dim)\n",
    "                    pkl_utils._save(os.path.join(self.feat_dir, fname+config.FEAT_FILE_SUFFIX), x[:,i])\n",
    "                    corr = np_utils._corr(x[:TRAIN_SIZE,i], y_train)\n",
    "                    self.logger.info(\"%s (%dD): corr = %.6f\"%(fname, dim, corr))\n",
    "            else:\n",
    "                dim = np_utils._dim(x)\n",
    "                fname = \"%s_%s_%dD\"%(ext.__name__(), obs_field, dim)\n",
    "                pkl_utils._save(os.path.join(self.feat_dir, fname+config.FEAT_FILE_SUFFIX), x)\n",
    "                if dim == 1:\n",
    "                    corr = np_utils._corr(x[:TRAIN_SIZE], y_train)\n",
    "                    self.logger.info(\"%s (%dD): corr = %.6f\"%(fname, dim, corr))\n",
    "                elif self.force_corr:\n",
    "                    for j in range(dim):\n",
    "                        corr = np_utils._corr(x[:TRAIN_SIZE,j], y_train)\n",
    "                        self.logger.info(\"%s (%d/%dD): corr = %.6f\"%(fname, j+1, dim, corr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "shared-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for generating pairwise feature, e.g., \n",
    "# intersect count of words between search_term and product_title\n",
    "class PairwiseFeatureWrapper:\n",
    "    def __init__(self, generator, dfAll, obs_fields, target_fields, param_list, feat_dir, logger, force_corr=False):\n",
    "        self.generator = generator\n",
    "        self.dfAll = dfAll\n",
    "        self.obs_fields = obs_fields\n",
    "        self.target_fields = target_fields\n",
    "        self.param_list = param_list\n",
    "        self.feat_dir = feat_dir\n",
    "        self.logger = logger\n",
    "        self.force_corr = force_corr\n",
    "\n",
    "    def go(self):\n",
    "        y_train = self.dfAll[\"relevance\"].values[:TRAIN_SIZE]\n",
    "        for obs_field in self.obs_fields:\n",
    "            if obs_field not in self.dfAll.columns:\n",
    "                self.logger.info(\"Skip %s\"%obs_field)\n",
    "                continue\n",
    "            obs_corpus = self.dfAll[obs_field].values\n",
    "            for target_field in self.target_fields:\n",
    "                if target_field not in self.dfAll.columns:\n",
    "                    self.logger.info(\"Skip %s\"%target_field)\n",
    "                    continue\n",
    "                target_corpus = self.dfAll[target_field].values\n",
    "                ext = self.generator(obs_corpus, target_corpus, *self.param_list)\n",
    "                x = ext.transform()\n",
    "                if isinstance(ext.__name__(), list):\n",
    "                    for i,feat_name in enumerate(ext.__name__()):\n",
    "                        dim = 1\n",
    "                        fname = \"%s_%s_x_%s_%dD\"%(feat_name, obs_field, target_field, dim)\n",
    "                        pkl_utils._save(os.path.join(self.feat_dir, fname+config.FEAT_FILE_SUFFIX), x[:,i])\n",
    "                        corr = np_utils._corr(x[:TRAIN_SIZE,i], y_train)\n",
    "                        self.logger.info(\"%s (%dD): corr = %.6f\"%(fname, dim, corr))\n",
    "                else:\n",
    "                    dim = np_utils._dim(x)\n",
    "                    fname = \"%s_%s_x_%s_%dD\"%(ext.__name__(), obs_field, target_field, dim)\n",
    "                    pkl_utils._save(os.path.join(self.feat_dir, fname+config.FEAT_FILE_SUFFIX), x)\n",
    "                    if dim == 1:\n",
    "                        corr = np_utils._corr(x[:TRAIN_SIZE], y_train)\n",
    "                        self.logger.info(\"%s (%dD): corr = %.6f\"%(fname, dim, corr))\n",
    "                    elif self.force_corr:\n",
    "                        for j in range(dim):\n",
    "                            corr = np_utils._corr(x[:TRAIN_SIZE,j], y_train)\n",
    "                            self.logger.info(\"%s (%d/%dD): corr = %.6f\"%(fname, j+1, dim, corr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forbidden-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading feature_base.ipynb in format ipynb\n",
      "[jupytext] Writing feature_base.py\n"
     ]
    }
   ],
   "source": [
    "# convert notebook.ipynb to a .py file\n",
    "!jupytext --to py feature_base.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
