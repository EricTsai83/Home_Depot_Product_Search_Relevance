{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Basic Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can return some system information\n",
    "import platform\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<strong>Note!</strong> Sometimes, we need to test our process work or not. So we need to test small sample and make sure the programme run smoothly and correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Overall -----------------------\n",
    "TASK = 'sample'  #'all'\n",
    "# # for testing data processing and feature generation\n",
    "# TASK = \"sample\"\n",
    "SAMPLE_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size\n",
    "TRAIN_SIZE = 74067\n",
    "if TASK == 'sample':\n",
    "    TRAIN_SIZE = SAMPLE_SIZE\n",
    "TEST_SIZE = 166693\n",
    "VALID_SIZE_MAX = 60000 # 0.7 * TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ PATH ------------------------\n",
    "# Root directory\n",
    "ROOT_DIR = '..' \n",
    "\n",
    "# Subdirectory\n",
    "\n",
    "## data\n",
    "DATA_DIR = f'{ROOT_DIR}/Data'\n",
    "### clean data\n",
    "CLEAN_DATA_DIR = f'{DATA_DIR}/Clean'\n",
    "\n",
    "## Feature\n",
    "FEAT_DIR = f'{ROOT_DIR}/Feat'\n",
    "FEAT_FILE_SUFFIX = \".pkl\"\n",
    "\n",
    "## Code\n",
    "CODE_DIR = f'{ROOT_DIR}/Code'\n",
    "### feature config\n",
    "FEAT_CONF_DIR = f'{CODE_DIR}/conf'\n",
    "\n",
    "## Figure\n",
    "FIG_DIR = f'{ROOT_DIR}/Fig'\n",
    "\n",
    "## Log\n",
    "LOG_DIR = f'{ROOT_DIR}/Log'\n",
    "\n",
    "## output \n",
    "OUTPUT_DIR = f'{ROOT_DIR}/Output'\n",
    "### submit output\n",
    "SUBM_DIR = f'{OUTPUT_DIR}/Subm'\n",
    "\n",
    "## Temporary folder\n",
    "TMP_DIR = f'{ROOT_DIR}/Tmp'\n",
    "\n",
    "## Thirdparty folder\n",
    "THIRDPARTY_DIR = f'{ROOT_DIR}/Thirdparty'\n",
    "\n",
    "# dictionary\n",
    "WORD_REPLACER_DATA = \"%s/dict/word_replacer.csv\"%DATA_DIR\n",
    "\n",
    "# colors\n",
    "COLOR_DATA = \"%s/dict/color_data.py\"%DATA_DIR\n",
    "\n",
    "\n",
    "# index split\n",
    "SPLIT_DIR = \"%s/split\"%DATA_DIR\n",
    "\n",
    "# ------------------------ DATA ------------------------\n",
    "# provided data\n",
    "TRAIN_DATA = f'{DATA_DIR}/train.csv'\n",
    "TEST_DATA = f'{DATA_DIR}/test.csv'\n",
    "ATTR_DATA = f'{DATA_DIR}/attributes.csv'\n",
    "DESC_DATA = f'{DATA_DIR}/product_descriptions.csv'\n",
    "SAMPLE_DATA = f'{DATA_DIR}/sample_submission.csv'\n",
    "\n",
    "ALL_DATA_RAW = f'{CLEAN_DATA_DIR}/all.raw.csv.pkl'\n",
    "INFO_DATA = f'{CLEAN_DATA_DIR}/info.csv.pkl'\n",
    "ALL_DATA_LEMMATIZED = f'{CLEAN_DATA_DIR}/all.lemmatized.csv.pkl'\n",
    "ALL_DATA_LEMMATIZED_STEMMED = f'{CLEAN_DATA_DIR}/all.lemmatized.stemmed.csv.pkl'\n",
    "\n",
    "\n",
    "# ------------------------ PARAM ------------------------\n",
    "# missing value\n",
    "MISSING_VALUE_STRING = 'MISSINGVALUE'   # str type\n",
    "MISSING_VALUE_NUMERIC = -1.   # float type\n",
    "\n",
    "# attribute name and value SEPARATOR\n",
    "ATTR_SEPARATOR = \" | \"\n",
    "\n",
    "\n",
    "# correct query with google spelling check dict\n",
    "# turn this on/off to have two versions of features/models\n",
    "# which is useful for ensembling, but not used in final submission\n",
    "GOOGLE_CORRECTING_QUERY = False\n",
    "\n",
    "\n",
    "# auto correcting query (quite time consuming; not used in final submission)\n",
    "AUTO_CORRECTING_QUERY = False \n",
    "\n",
    "\n",
    "# query expansion (not used in final submission)\n",
    "QUERY_EXPANSION = False\n",
    "\n",
    "# stop words\n",
    "STOP_WORDS = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# cv\n",
    "N_RUNS = 5\n",
    "N_FOLDS = 1\n",
    "\n",
    "\n",
    "# intersect count/match\n",
    "STR_MATCH_THRESHOLD = 0.85\n",
    "\n",
    "# ------------------------ OTHER ------------------------\n",
    "PLATFORM = platform.system()\n",
    "\n",
    "DATA_PROCESSOR_N_JOBS = 2 if PLATFORM == \"Windows\" else 6  # my notebook only two core. So sad.\n",
    "\n",
    "AUTO_SPELLING_CHECKER_N_JOBS = 4 if PLATFORM == \"Windows\" else 8\n",
    "# multi processing is not faster\n",
    "AUTO_SPELLING_CHECKER_N_JOBS = 1\n",
    "\n",
    "RANDOM_SEED = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert notebook to python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading config.ipynb in format ipynb\n",
      "[jupytext] Writing config.py (destination file replaced)\n"
     ]
    }
   ],
   "source": [
    "# convert notebook.ipynb to a .py file\n",
    "!jupytext --to py config.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
